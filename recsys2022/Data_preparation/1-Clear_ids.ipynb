{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "base_path = '../dataset'\n",
    "\n",
    "original_data = os.path.join(base_path, 'original_data')\n",
    "processed_data = os.path.join(base_path, 'processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [19],\n",
       "       [20]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_items=pd.read_csv(os.path.join(original_data, \"candidate_items.csv\"),usecols=[\"item_id\"]).drop_duplicates().sort_values(by=\"item_id\").to_numpy()\n",
    "candidate_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions_items=pd.read_csv(os.path.join(original_data,\"train_sessions.csv\"),usecols=[\"item_id\"]).drop_duplicates().sort_values(by=\"item_id\").to_numpy()\n",
    "train_sessions_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [4],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_purchases_items=pd.read_csv(os.path.join(original_data,\"train_purchases.csv\"),usecols=[\"item_id\"]).drop_duplicates().sort_values(by=\"item_id\").to_numpy()\n",
    "train_purchases_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [19]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions_items=pd.read_csv(os.path.join(original_data,\"test_final_sessions.csv\"),usecols=[\"item_id\"]).drop_duplicates().sort_values(by=\"item_id\").to_numpy()\n",
    "test_sessions_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [19],\n",
       "       [20]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions_leaderboard_items=pd.read_csv(os.path.join(original_data,\"test_leaderboard_sessions.csv\"),usecols=[\"item_id\"]).drop_duplicates().sort_values(by=\"item_id\").to_numpy()\n",
    "test_sessions_leaderboard_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_test_items = np.concatenate((test_sessions_leaderboard_items, test_sessions_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  313,   366,   575,  1152,  1364,  1883,  2497,  2523,  2677,\n",
       "        2694,  3185,  3529,  3754,  3835,  4042,  4514,  5214,  5394,\n",
       "        6171,  6853,  6873,  6916,  7204,  7780,  8758,  8771,  9384,\n",
       "        9418,  9589, 10463, 10671, 11125, 11933, 12667, 13376, 13618,\n",
       "       13943, 13972, 14395, 14622, 14723, 14967, 15601, 15629, 16206,\n",
       "       17046, 17206, 18482, 18690, 18837, 19637, 21241, 21444, 21904,\n",
       "       21927, 21998, 22030, 22316, 22703, 22746, 24303, 25035, 25277,\n",
       "       25521, 26232, 26742, 27377, 27728, 27826])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InTestButNotInTrain=np.setdiff1d(total_test_items,train_sessions_items)\n",
    "InTestButNotInTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69  items are in test sessions but not in train\n"
     ]
    }
   ],
   "source": [
    "print(len(InTestButNotInTrain),\" items are in test sessions but not in train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(InTestButNotInTrain,train_purchases_items)\n",
    "#None of those 67 items is seen in the train_purchases, so they are unknown items, should be treated in a special way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  221,   313,   344,   366,   575,   950,  1152,  1222,  1364,\n",
       "        1883,  1935,  2411,  2489,  2497,  2523,  2621,  2677,  2694,\n",
       "        3185,  3220,  3529,  3754,  3835,  4042,  4514,  6171,  6425,\n",
       "        6512,  6853,  6873,  6916,  7204,  7321,  7408,  7710,  7780,\n",
       "        8053,  8758,  8814,  8839,  8928,  9384,  9418,  9589,  9974,\n",
       "       10463, 10491, 10496, 10671, 10694, 11125, 11862, 11933, 12438,\n",
       "       12641, 12667, 13376, 13550, 13618, 13788, 13943, 13972, 14167,\n",
       "       14395, 14535, 14723, 15629, 15719, 16045, 16092, 16206, 16800,\n",
       "       17046, 17057, 17206, 17371, 17534, 17576, 17950, 18837, 19808,\n",
       "       20237, 21056, 21241, 21444, 21599, 21904, 21998, 22096, 22143,\n",
       "       22316, 22583, 22703, 22746, 22838, 23640, 23981, 24239, 24303,\n",
       "       25035, 25179, 25277, 25298, 25521, 26201, 26232, 26742, 27042,\n",
       "       27377, 27427, 27728, 27826, 27847])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CandidateNeverBoughtInTrainingSet=np.setdiff1d(candidate_items,train_purchases_items)\n",
    "CandidateNeverBoughtInTrainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CandidateNeverBoughtInTrainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  221,   344,   950,  1222,  1935,  2411,  2489,  2621,  3220,\n",
       "        6425,  6512,  7321,  7408,  7710,  8053,  8839,  8928,  9974,\n",
       "       10491, 10496, 10694, 11862, 12438, 12641, 13550, 13788, 14167,\n",
       "       14535, 15719, 16045, 16092, 16800, 17057, 17371, 17534, 17950,\n",
       "       19808, 21056, 21599, 22096, 22143, 22583, 22838, 23640, 23981,\n",
       "       24239, 25179, 25298, 27042, 27427, 27847])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SeenInTrainSessions=np.intersect1d(CandidateNeverBoughtInTrainingSet,train_sessions_items)\n",
    "SeenInTrainSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SeenInTrainSessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data=np.concatenate((candidate_items,train_purchases_items,train_sessions_items,test_sessions_items,test_sessions_leaderboard_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=[\"item_id\"],data=data,index=[i for i,_ in enumerate(data)]).drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23691 entries, 0 to 23690\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   index    23691 non-null  int64\n",
      " 1   item_id  23691 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 370.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"code\"]=df.index\n",
    "df[\"code\"]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(processed_data,\"map.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:19021].to_csv(os.path.join(processed_data,\"map_purchases.csv\"),index=False) # Mapping of only bought items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3866861342.py:1: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  train_sessions=pd.read_csv(os.path.join(original_data,\"train_sessions.csv\"),parse_dates=['date'],\n"
     ]
    }
   ],
   "source": [
    "train_sessions=pd.read_csv(os.path.join(original_data,\"train_sessions.csv\"),parse_dates=['date'], \n",
    "                         infer_datetime_format=True,header=0)\n",
    "                         \n",
    "train_sessions_ids=train_sessions.sort_values(\"date\").drop_duplicates(\"session_id\",keep='first')[\"session_id\"]\n",
    "train_sessions_ids=train_sessions_ids.reset_index()\n",
    "train_sessions_ids[\"index\"]=train_sessions_ids.index\n",
    "train_sessions_ids.to_csv(os.path.join(processed_data,\"sessions_map.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_sessions=pd.read_csv(os.path.join(original_data,\"test_final_sessions.csv\"))\n",
    "test_leaderboard_sessions=pd.read_csv(os.path.join(original_data,\"test_leaderboard_sessions.csv\"))\n",
    "test_sessions=pd.concat([test_final_sessions,test_leaderboard_sessions],axis=0)\n",
    "\n",
    "test_sessions_ids=test_sessions.sort_values(\"date\").drop_duplicates(\"session_id\",keep='first')[\"session_id\"]\n",
    "test_sessions_ids=test_sessions_ids.reset_index()\n",
    "test_sessions_ids[\"index\"]=test_sessions_ids.index\n",
    "test_sessions_ids.to_csv(os.path.join(processed_data,\"test_sessions_map.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_items=pd.read_csv(os.path.join(original_data,\"candidate_items.csv\"))\n",
    "candidate_items=candidate_items.merge(df,on=\"item_id\",how=\"left\")\n",
    "candidate_items[\"item_id\"]=candidate_items[\"code\"]\n",
    "del candidate_items[\"code\"]\n",
    "candidate_items.to_csv(os.path.join(processed_data,\"candidate_items_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_sessions=pd.read_csv(os.path.join(original_data,\"test_final_sessions.csv\"))\n",
    "test_final_sessions=test_final_sessions.merge(df,on=\"item_id\",how=\"left\")\n",
    "test_final_sessions[\"item_id\"]=test_final_sessions[\"code\"]\n",
    "del test_final_sessions[\"code\"]\n",
    "test_final_sessions=test_final_sessions.merge(test_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "test_final_sessions[\"session_id\"]=test_final_sessions[\"index\"]\n",
    "del test_final_sessions[\"index\"]\n",
    "test_final_sessions.to_csv(os.path.join(processed_data,\"test_final_sessions_full_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_sessions=pd.read_csv(os.path.join(original_data,\"test_final_sessions.csv\"))\n",
    "test_final_sessions=test_final_sessions.merge(df,on=\"item_id\",how=\"left\")\n",
    "test_final_sessions[\"item_id\"]=test_final_sessions[\"code\"]\n",
    "del test_final_sessions[\"code\"]\n",
    "test_final_sessions.to_csv(os.path.join(processed_data,\"test_final_sessions_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_leaderboard_sessions=pd.read_csv(os.path.join(original_data,\"test_leaderboard_sessions.csv\"))\n",
    "test_leaderboard_sessions=test_leaderboard_sessions.merge(df,on=\"item_id\",how=\"left\")\n",
    "test_leaderboard_sessions[\"item_id\"]=test_leaderboard_sessions[\"code\"]\n",
    "del test_leaderboard_sessions[\"code\"]\n",
    "test_leaderboard_sessions=test_leaderboard_sessions.merge(test_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "test_leaderboard_sessions[\"session_id\"]=test_leaderboard_sessions[\"index\"]\n",
    "del test_leaderboard_sessions[\"index\"]\n",
    "test_leaderboard_sessions.to_csv(os.path.join(processed_data,\"test_leaderboard_sessions_full_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_leaderboard_sessions=pd.read_csv(os.path.join(original_data,\"test_leaderboard_sessions.csv\"))\n",
    "test_leaderboard_sessions=test_leaderboard_sessions.merge(df,on=\"item_id\",how=\"left\")\n",
    "test_leaderboard_sessions[\"item_id\"]=test_leaderboard_sessions[\"code\"]\n",
    "del test_leaderboard_sessions[\"code\"]\n",
    "test_leaderboard_sessions.to_csv(os.path.join(processed_data,\"test_leaderboard_sessions_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchases=pd.read_csv(os.path.join(original_data,\"train_purchases.csv\"))\n",
    "train_purchases=train_purchases.merge(df,on=\"item_id\",how=\"left\")\n",
    "train_purchases[\"item_id\"]=train_purchases[\"code\"]\n",
    "del train_purchases[\"code\"]\n",
    "train_purchases=train_purchases.merge(train_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "train_purchases[\"session_id\"]=train_purchases[\"index\"]\n",
    "del train_purchases[\"index\"]\n",
    "train_purchases.to_csv(os.path.join(processed_data,\"train_purchases_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions=pd.read_csv(os.path.join(original_data,\"train_sessions.csv\"))\n",
    "train_sessions=train_sessions.merge(df,on=\"item_id\",how=\"left\")\n",
    "train_sessions[\"item_id\"]=train_sessions[\"code\"]\n",
    "del train_sessions[\"code\"]\n",
    "train_sessions=train_sessions.merge(train_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "train_sessions[\"session_id\"]=train_sessions[\"index\"]\n",
    "del train_sessions[\"index\"]\n",
    "train_sessions.to_csv(os.path.join(processed_data,\"train_sessions_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "item_features=pd.read_csv(os.path.join(original_data,\"item_features.csv\"))\n",
    "item_features=item_features.merge(df,on=\"item_id\",how=\"left\")\n",
    "item_features[\"item_id\"]=item_features[\"code\"]\n",
    "del item_features[\"code\"]\n",
    "item_features.to_csv(os.path.join(processed_data,\"item_features_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2766656796.py:1: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  train_sessions=pd.read_csv(os.path.join(original_data,\"train_sessions.csv\"),parse_dates=['date'],\n"
     ]
    }
   ],
   "source": [
    "train_sessions=pd.read_csv(os.path.join(original_data,\"train_sessions.csv\"),parse_dates=['date'], \n",
    "                         infer_datetime_format=True,header=0)\n",
    "\n",
    "\n",
    "last_month_start = datetime.strptime(\"01/05/21 00:00:00\", '%d/%m/%y %H:%M:%S')\n",
    "# Remove the miliseconds to avoid format compitabilities\n",
    "train_sessions[\"date\"] = [x.split(\".\")[0] for x in train_sessions[\"date\"]]\n",
    "train_sessions[\"date\"] = pd.to_datetime(train_sessions[\"date\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "train_sessions_train_split=train_sessions[train_sessions[\"date\"]<last_month_start]\n",
    "\n",
    "\n",
    "train_sessions_train_split=train_sessions_train_split.merge(df,on=\"item_id\",how=\"left\")\n",
    "train_sessions_train_split[\"item_id\"]=train_sessions_train_split[\"code\"]\n",
    "del train_sessions_train_split[\"code\"]\n",
    "train_sessions_train_split=train_sessions_train_split.merge(train_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "train_sessions_train_split[\"session_id\"]=train_sessions_train_split[\"index\"]\n",
    "del train_sessions_train_split[\"index\"]\n",
    "train_sessions_train_split.to_csv(os.path.join(processed_data,\"train_sessions_train_split_mapped.csv\"),index=False)\n",
    "\n",
    "train_sessions_valid_split=train_sessions[train_sessions[\"date\"]>=last_month_start]\n",
    "\n",
    "\n",
    "train_sessions_valid_split=train_sessions_valid_split.merge(df,on=\"item_id\",how=\"left\")\n",
    "train_sessions_valid_split[\"item_id\"]=train_sessions_valid_split[\"code\"]\n",
    "del train_sessions_valid_split[\"code\"]\n",
    "train_sessions_valid_split=train_sessions_valid_split.merge(train_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "train_sessions_valid_split[\"session_id\"]=train_sessions_valid_split[\"index\"]\n",
    "del train_sessions_valid_split[\"index\"]\n",
    "train_sessions_valid_split.to_csv(os.path.join(processed_data,\"train_sessions_valid_split_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/476225623.py:1: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  train_purchases=pd.read_csv(os.path.join(original_data,\"train_purchases.csv\"),parse_dates=['date'],\n"
     ]
    }
   ],
   "source": [
    "train_purchases=pd.read_csv(os.path.join(original_data,\"train_purchases.csv\"),parse_dates=['date'], \n",
    "                         infer_datetime_format=True,header=0)\n",
    "\n",
    "\n",
    "last_month_start = datetime.strptime(\"01/05/21 00:00:00\", '%d/%m/%y %H:%M:%S')\n",
    "# Remove the miliseconds to avoid format compitabilities\n",
    "train_purchases[\"date\"] = [x.split(\".\")[0] for x in train_purchases[\"date\"]]\n",
    "train_purchases[\"date\"] = pd.to_datetime(train_purchases[\"date\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "train_purchases_train_split=train_purchases[train_purchases[\"date\"]<last_month_start]\n",
    "\n",
    "\n",
    "train_purchases_train_split=train_purchases_train_split.merge(df,on=\"item_id\",how=\"left\")\n",
    "train_purchases_train_split[\"item_id\"]=train_purchases_train_split[\"code\"]\n",
    "del train_purchases_train_split[\"code\"]\n",
    "train_purchases_train_split=train_purchases_train_split.merge(train_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "train_purchases_train_split[\"session_id\"]=train_purchases_train_split[\"index\"]\n",
    "del train_purchases_train_split[\"index\"]\n",
    "train_purchases_train_split.to_csv(os.path.join(processed_data,\"train_purchases_train_split_mapped.csv\"),index=False)\n",
    "\n",
    "train_purchases_valid_split=train_purchases[train_purchases[\"date\"]>=last_month_start]\n",
    "\n",
    "\n",
    "train_purchases_valid_split=train_purchases_valid_split.merge(df,on=\"item_id\",how=\"left\")\n",
    "train_purchases_valid_split[\"item_id\"]=train_purchases_valid_split[\"code\"]\n",
    "del train_purchases_valid_split[\"code\"]\n",
    "train_purchases_valid_split=train_purchases_valid_split.merge(train_sessions_ids,on=\"session_id\",how=\"left\")\n",
    "train_purchases_valid_split[\"session_id\"]=train_purchases_valid_split[\"index\"]\n",
    "del train_purchases_valid_split[\"index\"]\n",
    "train_purchases_valid_split.to_csv(os.path.join(processed_data,\"train_purchases_valid_split_mapped.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_URM(file='train_sessions_train_split_mapped.csv',files_directory=\"../dataset/processed_data\", normalized=False,weight_decay=False):\n",
    "    df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
    "                         parse_dates=['date'], \n",
    "                         infer_datetime_format=True)\n",
    "    \n",
    "    \n",
    "    df_URM['count']=1\n",
    "    df_URM = df_URM.groupby([\"session_id\", \"item_id\"])[\"count\"].sum().reset_index()\n",
    "    session_id_list = df_URM['session_id'].values\n",
    "    item_id_list = df_URM['item_id'].values\n",
    "    \n",
    "    if not normalized:\n",
    "        rating_id_list = df_URM['count'].values\n",
    "    else:\n",
    "        rating_id_list = np.ones_like(session_id_list)\n",
    "\n",
    "\n",
    "    URM = sps.csr_matrix((rating_id_list, (session_id_list, item_id_list)),shape=(1000000,23692))\n",
    "\n",
    "    \n",
    "\n",
    "    return URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/1482187444.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/1482187444.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/1482187444.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/1482187444.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    }
   ],
   "source": [
    "train_sessions_seen = get_URM()\n",
    "train_bought = get_URM(\"train_purchases_train_split_mapped.csv\")\n",
    "\n",
    "valid_sessions_seen= get_URM(\"train_sessions_valid_split_mapped.csv\")\n",
    "valid_bought = get_URM(\"train_purchases_valid_split_mapped.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.save_npz(os.path.join(processed_data, \"URM_train_seen.npz\"), train_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_train_bought.npz\"), train_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_valid_seen.npz\"), valid_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_valid_bought.npz\"), valid_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_train_full.npz\"), train_sessions_seen+train_bought)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_valid_full.npz\"), valid_sessions_seen+valid_bought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_URM_test(file='train_sessions_train_split_mapped.csv',files_directory=\"../dataset/processed_data\", normalized=False,weight_decay=False):\n",
    "    df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
    "                         parse_dates=['date'], \n",
    "                         infer_datetime_format=True)\n",
    "    \n",
    "    \n",
    "    df_URM['count']=1\n",
    "    df_URM = df_URM.groupby([\"session_id\", \"item_id\"])[\"count\"].sum().reset_index()\n",
    "    session_id_list = df_URM['session_id'].values\n",
    "    item_id_list = df_URM['item_id'].values\n",
    "    \n",
    "    if not normalized:\n",
    "        rating_id_list = df_URM['count'].values\n",
    "    else:\n",
    "        rating_id_list = np.ones_like(session_id_list)\n",
    "\n",
    "\n",
    "    URM = sps.csr_matrix((rating_id_list, (session_id_list, item_id_list)))\n",
    "\n",
    "    \n",
    "\n",
    "    return URM,np.unique(session_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/1574550132.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/1574550132.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    }
   ],
   "source": [
    "test_leaderboard,leaderboard_sessions = get_URM_test(file=\"test_leaderboard_sessions_mapped.csv\")\n",
    "\n",
    "test_final,final_sessions = get_URM_test(file=\"test_final_sessions_mapped.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.save_npz(os.path.join(processed_data, \"URM_test_leaderboard.npz\"), test_leaderboard)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_test_final.npz\"), test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(processed_data, \"leaderboard_sessions_ids\"),leaderboard_sessions)\n",
    "np.save(os.path.join(processed_data, \"final_sessions_ids\"),final_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_URM_temporal_decay(file='train_sessions_train_split_mapped.csv',files_directory=\"../dataset/processed_data\", normalized=False,weight_decay=False,temperature=30,last_date=datetime.strptime(\"01/05/21 00:00:00\", '%d/%m/%y %H:%M:%S')):\n",
    "    df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
    "                         parse_dates=['date'], \n",
    "                         infer_datetime_format=True)\n",
    "    df_URM[\"diff\"] = last_date-df_URM[\"date\"]\n",
    "    df_URM[\"days\"] = df_URM[\"diff\"].dt.days\n",
    "    df_URM[\"days\"] = df_URM[\"days\"].apply(lambda x: max(0,x))\n",
    "    df_URM[\"count\"] = df_URM[\"days\"].apply(lambda x: np.exp(-x/temperature))\n",
    "    print(df_URM.head(10))\n",
    "\n",
    "\n",
    "    df_URM = df_URM.groupby([\"session_id\", \"item_id\"])[\"count\"].sum().reset_index()\n",
    "    session_id_list = df_URM['session_id'].values\n",
    "    item_id_list = df_URM['item_id'].values\n",
    "    \n",
    "    if not normalized:\n",
    "        rating_id_list = df_URM['count'].values\n",
    "    else:\n",
    "        rating_id_list = np.ones_like(session_id_list)\n",
    "\n",
    "\n",
    "    URM = sps.csr_matrix((rating_id_list, (session_id_list, item_id_list)),shape=(1000000,23692))\n",
    "\n",
    "    \n",
    "    print(\"computed\")\n",
    "    return URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date              diff  days     count\n",
      "0      663204     9769 2020-12-18 21:25:00 133 days 02:35:00   133  0.011875\n",
      "1      663204     9769 2020-12-18 21:19:48 133 days 02:40:12   133  0.011875\n",
      "2       85375    12783 2020-03-13 19:35:27 413 days 04:24:33   413  0.000001\n",
      "3      374472    14116 2020-08-26 19:18:30 247 days 04:41:30   247  0.000266\n",
      "4      374472     6166 2020-08-26 19:16:31 247 days 04:43:29   247  0.000266\n",
      "5      374472     6924 2020-08-26 19:15:47 247 days 04:44:13   247  0.000266\n",
      "6      526578     4546 2020-11-02 16:31:18 179 days 07:28:42   179  0.002563\n",
      "7      526578     8092 2020-11-02 16:34:33 179 days 07:25:27   179  0.002563\n",
      "8      526578    17752 2020-11-02 16:43:04 179 days 07:16:56   179  0.002563\n",
      "9      526578    14975 2020-11-02 16:42:02 179 days 07:17:58   179  0.002563\n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date              diff  days  \\\n",
      "0      663204    12492 2020-12-18 21:26:47 133 days 02:33:13   133   \n",
      "1       85375     3299 2020-03-13 19:36:15 413 days 04:23:45   413   \n",
      "2      374472    17411 2020-08-26 19:20:32 247 days 04:39:28   247   \n",
      "3      526578    11229 2020-11-02 17:16:45 179 days 06:43:15   179   \n",
      "4       66630    11568 2020-02-26 18:27:44 429 days 05:32:16   429   \n",
      "5      192688    18172 2020-05-18 12:52:09 347 days 11:07:51   347   \n",
      "6      895749     1512 2021-04-20 19:46:42  10 days 04:13:18    10   \n",
      "7      257246    12205 2020-06-21 10:33:22 313 days 13:26:38   313   \n",
      "8      789144     2124 2021-03-01 15:17:04  60 days 08:42:56    60   \n",
      "9      580394     6925 2020-11-27 20:46:08 154 days 03:13:52   154   \n",
      "\n",
      "          count  \n",
      "0  1.187484e-02  \n",
      "1  1.050056e-06  \n",
      "2  2.656494e-04  \n",
      "3  2.562770e-03  \n",
      "4  6.160116e-07  \n",
      "5  9.476773e-06  \n",
      "6  7.165313e-01  \n",
      "7  2.943479e-05  \n",
      "8  1.353353e-01  \n",
      "9  5.896871e-03  \n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date               diff  days  count\n",
      "0      929236      417 2021-05-05 13:19:15  -5 days +10:40:45     0    1.0\n",
      "1      929236     3054 2021-05-05 14:13:21  -5 days +09:46:39     0    1.0\n",
      "2      929236     2780 2021-05-05 13:19:54  -5 days +10:40:06     0    1.0\n",
      "3      929236    14829 2021-05-05 13:18:49  -5 days +10:41:11     0    1.0\n",
      "4      929236     3746 2021-05-05 14:03:43  -5 days +09:56:17     0    1.0\n",
      "5      929236     1139 2021-05-05 13:18:20  -5 days +10:41:40     0    1.0\n",
      "6      984653     2950 2021-05-27 10:14:06 -27 days +13:45:54     0    1.0\n",
      "7      984653     3491 2021-05-27 10:14:01 -27 days +13:45:59     0    1.0\n",
      "8      984653     2821 2021-05-27 10:13:55 -27 days +13:46:05     0    1.0\n",
      "9      984653     3491 2021-05-27 10:13:40 -27 days +13:46:20     0    1.0\n",
      "computed\n",
      "   session_id  item_id                date               diff  days  count\n",
      "0      929236     3732 2021-05-05 14:15:07  -5 days +09:44:53     0    1.0\n",
      "1      984653     4579 2021-05-27 10:24:05 -27 days +13:35:55     0    1.0\n",
      "2      998452     1605 2021-05-31 13:44:52 -31 days +10:15:08     0    1.0\n",
      "3      928718      806 2021-05-05 10:11:06  -5 days +13:48:54     0    1.0\n",
      "4      980028     4503 2021-05-25 16:24:30 -25 days +07:35:30     0    1.0\n",
      "5      970090     2620 2021-05-21 18:12:17 -21 days +05:47:43     0    1.0\n",
      "6      987395     1972 2021-05-28 08:35:35 -28 days +15:24:25     0    1.0\n",
      "7      952246     2725 2021-05-14 19:53:25 -14 days +04:06:35     0    1.0\n",
      "8      951522     1790 2021-05-14 17:48:41 -14 days +06:11:19     0    1.0\n",
      "9      961516     2483 2021-05-18 13:16:44 -18 days +10:43:16     0    1.0\n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    }
   ],
   "source": [
    "train_sessions_seen = get_URM_temporal_decay()\n",
    "train_bought = get_URM_temporal_decay(\"train_purchases_train_split_mapped.csv\")\n",
    "\n",
    "valid_sessions_seen= get_URM_temporal_decay(\"train_sessions_valid_split_mapped.csv\")\n",
    "valid_bought = get_URM_temporal_decay(\"train_purchases_valid_split_mapped.csv\")\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WT_train_seen.npz\"), train_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WT_train_bought.npz\"), train_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WT_valid_seen.npz\"), valid_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WT_valid_bought.npz\"), valid_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WT_train_full.npz\"), train_sessions_seen+train_bought)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WT_valid_full.npz\"), valid_sessions_seen+valid_bought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date              diff  days  \\\n",
      "0      663204     9769 2020-12-18 21:25:00 164 days 02:35:00   164   \n",
      "1      663204     9769 2020-12-18 21:19:48 164 days 02:40:12   164   \n",
      "2       85375    12783 2020-03-13 19:35:27 444 days 04:24:33   444   \n",
      "3      374472    14116 2020-08-26 19:18:30 278 days 04:41:30   278   \n",
      "4      374472     6166 2020-08-26 19:16:31 278 days 04:43:29   278   \n",
      "5      374472     6924 2020-08-26 19:15:47 278 days 04:44:13   278   \n",
      "6      526578     4546 2020-11-02 16:31:18 210 days 07:28:42   210   \n",
      "7      526578     8092 2020-11-02 16:34:33 210 days 07:25:27   210   \n",
      "8      526578    17752 2020-11-02 16:43:04 210 days 07:16:56   210   \n",
      "9      526578    14975 2020-11-02 16:42:02 210 days 07:17:58   210   \n",
      "\n",
      "          count  \n",
      "0  4.225293e-03  \n",
      "1  4.225293e-03  \n",
      "2  3.736299e-07  \n",
      "3  9.452307e-05  \n",
      "4  9.452307e-05  \n",
      "5  9.452307e-05  \n",
      "6  9.118820e-04  \n",
      "7  9.118820e-04  \n",
      "8  9.118820e-04  \n",
      "9  9.118820e-04  \n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date              diff  days  \\\n",
      "0      663204    12492 2020-12-18 21:26:47 164 days 02:33:13   164   \n",
      "1       85375     3299 2020-03-13 19:36:15 444 days 04:23:45   444   \n",
      "2      374472    17411 2020-08-26 19:20:32 278 days 04:39:28   278   \n",
      "3      526578    11229 2020-11-02 17:16:45 210 days 06:43:15   210   \n",
      "4       66630    11568 2020-02-26 18:27:44 460 days 05:32:16   460   \n",
      "5      192688    18172 2020-05-18 12:52:09 378 days 11:07:51   378   \n",
      "6      895749     1512 2021-04-20 19:46:42  41 days 04:13:18    41   \n",
      "7      257246    12205 2020-06-21 10:33:22 344 days 13:26:38   344   \n",
      "8      789144     2124 2021-03-01 15:17:04  91 days 08:42:56    91   \n",
      "9      580394     6925 2020-11-27 20:46:08 185 days 03:13:52   185   \n",
      "\n",
      "          count  \n",
      "0  4.225293e-03  \n",
      "1  3.736299e-07  \n",
      "2  9.452307e-05  \n",
      "3  9.118820e-04  \n",
      "4  2.191886e-07  \n",
      "5  3.372015e-06  \n",
      "6  2.549554e-01  \n",
      "7  1.047345e-05  \n",
      "8  4.815485e-02  \n",
      "9  2.098218e-03  \n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date             diff  days     count\n",
      "0      929236      417 2021-05-05 13:19:15 26 days 10:40:45    26  0.420350\n",
      "1      929236     3054 2021-05-05 14:13:21 26 days 09:46:39    26  0.420350\n",
      "2      929236     2780 2021-05-05 13:19:54 26 days 10:40:06    26  0.420350\n",
      "3      929236    14829 2021-05-05 13:18:49 26 days 10:41:11    26  0.420350\n",
      "4      929236     3746 2021-05-05 14:03:43 26 days 09:56:17    26  0.420350\n",
      "5      929236     1139 2021-05-05 13:18:20 26 days 10:41:40    26  0.420350\n",
      "6      984653     2950 2021-05-27 10:14:06  4 days 13:45:54     4  0.875173\n",
      "7      984653     3491 2021-05-27 10:14:01  4 days 13:45:59     4  0.875173\n",
      "8      984653     2821 2021-05-27 10:13:55  4 days 13:46:05     4  0.875173\n",
      "9      984653     3491 2021-05-27 10:13:40  4 days 13:46:20     4  0.875173\n",
      "computed\n",
      "   session_id  item_id                date             diff  days     count\n",
      "0      929236     3732 2021-05-05 14:15:07 26 days 09:44:53    26  0.420350\n",
      "1      984653     4579 2021-05-27 10:24:05  4 days 13:35:55     4  0.875173\n",
      "2      998452     1605 2021-05-31 13:44:52  0 days 10:15:08     0  1.000000\n",
      "3      928718      806 2021-05-05 10:11:06 26 days 13:48:54    26  0.420350\n",
      "4      980028     4503 2021-05-25 16:24:30  6 days 07:35:30     6  0.818731\n",
      "5      970090     2620 2021-05-21 18:12:17 10 days 05:47:43    10  0.716531\n",
      "6      987395     1972 2021-05-28 08:35:35  3 days 15:24:25     3  0.904837\n",
      "7      952246     2725 2021-05-14 19:53:25 17 days 04:06:35    17  0.567414\n",
      "8      951522     1790 2021-05-14 17:48:41 17 days 06:11:19    17  0.567414\n",
      "9      961516     2483 2021-05-18 13:16:44 13 days 10:43:16    13  0.648344\n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/2708709298.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    }
   ],
   "source": [
    "train_sessions_seen = get_URM_temporal_decay(last_date=datetime.strptime(\"01/06/21 00:00:00\", '%d/%m/%y %H:%M:%S'))\n",
    "train_bought = get_URM_temporal_decay(\"train_purchases_train_split_mapped.csv\",last_date=datetime.strptime(\"01/06/21 00:00:00\", '%d/%m/%y %H:%M:%S'))\n",
    "\n",
    "valid_sessions_seen= get_URM_temporal_decay(\"train_sessions_valid_split_mapped.csv\",last_date=datetime.strptime(\"01/06/21 00:00:00\", '%d/%m/%y %H:%M:%S'))\n",
    "valid_bought = get_URM_temporal_decay(\"train_purchases_valid_split_mapped.csv\",last_date=datetime.strptime(\"01/06/21 00:00:00\", '%d/%m/%y %H:%M:%S'))\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WV_train_seen.npz\"), train_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WV_train_bought.npz\"), train_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WV_valid_seen.npz\"), valid_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WV_valid_bought.npz\"), valid_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WV_train_full.npz\"), train_sessions_seen+train_bought)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WV_valid_full.npz\"), valid_sessions_seen+valid_bought)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_URM_temporal_decay_within_session(file='train_sessions_train_split_mapped.csv',files_directory=\"../dataset/processed_data\", normalized=False,weight_decay=False,temperature=5,seen=False):\n",
    "    df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
    "                         parse_dates=['date'], \n",
    "                         infer_datetime_format=True)\n",
    "    df_URM[\"idx\"] = df_URM.index\n",
    "\n",
    "    temp=df_URM.groupby([\"session_id\"])[\"idx\"].max().reset_index()\n",
    "    \n",
    "    temp.rename(columns={\"idx\":\"idx_max\"},inplace=True)\n",
    "    df_URM=df_URM.merge(temp,on=\"session_id\",how=\"left\")\n",
    "    df_URM[\"idx\"]=df_URM[\"idx_max\"]-df_URM[\"idx\"] + 1 if seen else 0\n",
    "    df_URM[\"count\"] = df_URM[\"idx\"].apply(lambda x: np.exp(-x/temperature))\n",
    "    print(df_URM.head(10))\n",
    "\n",
    "    df_URM = df_URM.groupby([\"session_id\", \"item_id\"])[\"count\"].sum().reset_index()\n",
    "    session_id_list = df_URM['session_id'].values\n",
    "    item_id_list = df_URM['item_id'].values\n",
    "    \n",
    "    if not normalized:\n",
    "        rating_id_list = df_URM['count'].values\n",
    "    else:\n",
    "        rating_id_list = np.ones_like(session_id_list)\n",
    "\n",
    "\n",
    "    URM = sps.csr_matrix((rating_id_list, (session_id_list, item_id_list)),shape=(1000000,23692))\n",
    "\n",
    "    print(\"computed\")\n",
    "\n",
    "    return URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3120854546.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date  idx  idx_max  count\n",
      "0      663204     9769 2020-12-18 21:25:00    0        1    1.0\n",
      "1      663204     9769 2020-12-18 21:19:48    0        1    1.0\n",
      "2       85375    12783 2020-03-13 19:35:27    0        2    1.0\n",
      "3      374472    14116 2020-08-26 19:18:30    0        5    1.0\n",
      "4      374472     6166 2020-08-26 19:16:31    0        5    1.0\n",
      "5      374472     6924 2020-08-26 19:15:47    0        5    1.0\n",
      "6      526578     4546 2020-11-02 16:31:18    0       22    1.0\n",
      "7      526578     8092 2020-11-02 16:34:33    0       22    1.0\n",
      "8      526578    17752 2020-11-02 16:43:04    0       22    1.0\n",
      "9      526578    14975 2020-11-02 16:42:02    0       22    1.0\n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3120854546.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date  idx  idx_max  count\n",
      "0      663204    12492 2020-12-18 21:26:47    0        0    1.0\n",
      "1       85375     3299 2020-03-13 19:36:15    0        1    1.0\n",
      "2      374472    17411 2020-08-26 19:20:32    0        2    1.0\n",
      "3      526578    11229 2020-11-02 17:16:45    0        3    1.0\n",
      "4       66630    11568 2020-02-26 18:27:44    0        4    1.0\n",
      "5      192688    18172 2020-05-18 12:52:09    0        5    1.0\n",
      "6      895749     1512 2021-04-20 19:46:42    0        6    1.0\n",
      "7      257246    12205 2020-06-21 10:33:22    0        7    1.0\n",
      "8      789144     2124 2021-03-01 15:17:04    0        8    1.0\n",
      "9      580394     6925 2020-11-27 20:46:08    0        9    1.0\n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3120854546.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                date  idx  idx_max  count\n",
      "0      929236      417 2021-05-05 13:19:15    0        5    1.0\n",
      "1      929236     3054 2021-05-05 14:13:21    0        5    1.0\n",
      "2      929236     2780 2021-05-05 13:19:54    0        5    1.0\n",
      "3      929236    14829 2021-05-05 13:18:49    0        5    1.0\n",
      "4      929236     3746 2021-05-05 14:03:43    0        5    1.0\n",
      "5      929236     1139 2021-05-05 13:18:20    0        5    1.0\n",
      "6      984653     2950 2021-05-27 10:14:06    0       13    1.0\n",
      "7      984653     3491 2021-05-27 10:14:01    0       13    1.0\n",
      "8      984653     2821 2021-05-27 10:13:55    0       13    1.0\n",
      "9      984653     3491 2021-05-27 10:13:40    0       13    1.0\n",
      "computed\n",
      "   session_id  item_id                date  idx  idx_max  count\n",
      "0      929236     3732 2021-05-05 14:15:07    0        0    1.0\n",
      "1      984653     4579 2021-05-27 10:24:05    0        1    1.0\n",
      "2      998452     1605 2021-05-31 13:44:52    0        2    1.0\n",
      "3      928718      806 2021-05-05 10:11:06    0        3    1.0\n",
      "4      980028     4503 2021-05-25 16:24:30    0        4    1.0\n",
      "5      970090     2620 2021-05-21 18:12:17    0        5    1.0\n",
      "6      987395     1972 2021-05-28 08:35:35    0        6    1.0\n",
      "7      952246     2725 2021-05-14 19:53:25    0        7    1.0\n",
      "8      951522     1790 2021-05-14 17:48:41    0        8    1.0\n",
      "9      961516     2483 2021-05-18 13:16:44    0        9    1.0\n",
      "computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3120854546.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    }
   ],
   "source": [
    "train_sessions_seen = get_URM_temporal_decay_within_session()\n",
    "train_bought = get_URM_temporal_decay_within_session(\"train_purchases_train_split_mapped.csv\")\n",
    "\n",
    "valid_sessions_seen= get_URM_temporal_decay_within_session(\"train_sessions_valid_split_mapped.csv\")\n",
    "valid_bought = get_URM_temporal_decay_within_session(\"train_purchases_valid_split_mapped.csv\")\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_train_seen.npz\"), train_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_train_bought.npz\"), train_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_valid_seen.npz\"), valid_sessions_seen)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_valid_bought.npz\"), valid_bought)\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_train_full.npz\"), train_sessions_seen+train_bought)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_valid_full.npz\"), valid_sessions_seen+valid_bought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions_seen[:,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_URM_test_temporal_decay_within_session(file='train_sessions_train_split_mapped.csv',files_directory=\"../dataset/processed_data\", normalized=False,weight_decay=False,temperature=5):\n",
    "    df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n",
    "                         parse_dates=['date'], \n",
    "                         infer_datetime_format=True)\n",
    "    \n",
    "    \n",
    "    df_URM[\"idx\"] = df_URM.index\n",
    "\n",
    "    temp=df_URM.groupby([\"session_id\"])[\"idx\"].max().reset_index()\n",
    "    \n",
    "    temp.rename(columns={\"idx\":\"idx_max\"},inplace=True)\n",
    "    df_URM=df_URM.merge(temp,on=\"session_id\",how=\"left\")\n",
    "    df_URM[\"idx\"]=df_URM[\"idx_max\"]-df_URM[\"idx\"] + 1 \n",
    "    df_URM[\"count\"] = df_URM[\"idx\"].apply(lambda x: np.exp(-x/temperature))\n",
    "\n",
    "    print(df_URM.head(10))\n",
    "\n",
    "    df_URM = df_URM.groupby([\"session_id\", \"item_id\"])[\"count\"].sum().reset_index()\n",
    "    session_id_list = df_URM['session_id'].values\n",
    "    item_id_list = df_URM['item_id'].values\n",
    "    \n",
    "    if not normalized:\n",
    "        rating_id_list = df_URM['count'].values\n",
    "    else:\n",
    "        rating_id_list = np.ones_like(session_id_list)\n",
    "\n",
    "\n",
    "    URM = sps.csr_matrix((rating_id_list, (session_id_list, item_id_list)))\n",
    "\n",
    "    \n",
    "\n",
    "    return URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3521514054.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                     date  idx  idx_max     count\n",
      "0          26     3404  2021-06-16 09:53:54.158    1        0  0.818731\n",
      "1         200     3037  2021-06-25 12:23:40.811    4        4  0.449329\n",
      "2         200     3037  2021-06-25 12:24:36.631    3        4  0.548812\n",
      "3         200     1468  2021-06-25 12:24:41.677    2        4  0.670320\n",
      "4         200      887  2021-06-25 12:24:50.692    1        4  0.818731\n",
      "5         205     1484  2021-06-11 00:28:07.058    1        5  0.818731\n",
      "6         495     1250  2021-06-14 22:13:06.741    1        6  0.818731\n",
      "7         521     4673   2021-06-19 13:50:03.09    1        7  0.818731\n",
      "8         587     3945   2021-06-01 16:43:22.80    1        8  0.818731\n",
      "9         721     2464  2021-06-19 18:46:57.263    1        9  0.818731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/h_zrk_k931s4wj_999xrfy0m0000gq/T/ipykernel_65081/3521514054.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_URM = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, file), sep=',', header=0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                     date  idx  idx_max     count\n",
      "0          61     4785  2021-06-01 08:12:39.664    1        0  0.818731\n",
      "1          96     2107  2021-06-19 17:48:05.227    5        5  0.367879\n",
      "2          96     3251  2021-06-19 17:49:08.589    4        5  0.449329\n",
      "3          96      884  2021-06-19 17:49:15.838    3        5  0.548812\n",
      "4          96       89   2021-06-19 17:49:20.88    2        5  0.670320\n",
      "5          96     1252  2021-06-19 17:56:21.317    1        5  0.818731\n",
      "6         185     3133   2021-06-07 15:53:21.64    5       10  0.367879\n",
      "7         185     3767  2021-06-07 15:53:29.483    4       10  0.449329\n",
      "8         185     3767  2021-06-07 15:53:53.069    3       10  0.548812\n",
      "9         185     3133  2021-06-07 15:54:07.491    2       10  0.670320\n"
     ]
    }
   ],
   "source": [
    "test_leaderboard = get_URM_test_temporal_decay_within_session(file=\"test_leaderboard_sessions_mapped.csv\")\n",
    "\n",
    "test_final = get_URM_test_temporal_decay_within_session(file=\"test_final_sessions_mapped.csv\")\n",
    "\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_test_leaderboard.npz\"), test_leaderboard)\n",
    "sps.save_npz(os.path.join(processed_data, \"URM_WW_test_final.npz\"), test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.027912187333975"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_leaderboard[:,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__==\"2.15.0\",\"tf_version is wrong\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "61f38fddaed46afda7c40a5798d4631266e18af1d335aa6f772dc3ed9b8ab549"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
