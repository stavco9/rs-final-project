{"cells":[{"cell_type":"markdown","metadata":{},"source":["# This notebook uses the code available in the repository of the Recommender systems course at Politecnico di Milano\n","\n","We adapt the KNN User Collaborative Filtering Recommender to remove temporal data leakage.\n","We change the code so that we compute for every test session s' taken independently the k closest sessions s only from the train set. \n","\n","We compute so only a part of the similarity matrix, in particular we consider only the part that computes similarities of the test sessions with the train sessions. \n","\n","We don't compute similarities among the train sessions and other train sessions (which wouldn't help in predicting the test month) and also we don't compute similarities between test sessions and other test sessions (which would lead to leakages that are against the rules).\n","\n","We also modify the code of Top Popular recommender to use only a slice of all the available sessions, this is done to allow the use of only the sessions closer to the test month.\n","This is done to allow the use of these recommenders as we would use any other recommender of the repository.\n","\n","Item similarity based recommenders are trained only using train sessions and features provided for each item.\n","The inference uses the learned similarity matrix between the items, learned on train sessions, on the test sessions to suggest items close to those seen in the currently considered test session according to what can be learned in the train sessions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["just_checking_integrity=False\n","boundary=900000\n","boundary_after=-75000"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%capture\n","!git clone https://github.com/MaurizioFD/RecSys_Course_AT_PoliMi.git"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"./RecSys_Course_AT_PoliMi\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd ./RecSys_Course_AT_PoliMi"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile ./run_compile_all_cython.py\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on 30/03/2019\n","\n","@author: Maurizio Ferrari Dacrema\n","\"\"\"\n","\n","import sys, glob, traceback, os\n","from CythonCompiler.run_compile_subprocess import run_compile_subprocess\n","\n","\n","if __name__ == '__main__':\n","\n","    # cython_file_list = glob.glob('**/*.pyx', recursive=True)\n","\n","    subfolder_to_compile_list = [\n","        \"./Recommenders/Similarity\",\n","    ]\n","\n","\n","    cython_file_list = []\n","\n","    for subfolder_to_compile in subfolder_to_compile_list:\n","        cython_file_list.extend(glob.glob('{}/Cython/*.pyx'.format(subfolder_to_compile), recursive=True))\n","    \n","\n","\n","    print(\"run_compile_all_cython: Found {} Cython files in {} folders...\".format(len(cython_file_list), len(subfolder_to_compile_list)))\n","    print(\"run_compile_all_cython: All files will be compiled using your current python environment: '{}'\".format(sys.executable))\n","\n","\n","    save_folder_path = \"./result_cython_compile/\"\n","    log_file_path = save_folder_path + \"log.txt\"\n","\n","    # If directory does not exist, create\n","    if not os.path.exists(save_folder_path):\n","        os.makedirs(save_folder_path)\n","\n","\n","    log_file = open(log_file_path, \"w\")\n","\n","    fail_count = 0\n","\n","    for file_index, file_path in enumerate(cython_file_list):\n","\n","        file_path = file_path.replace(\"\\\\\", \"/\").split(\"/\")\n","\n","        file_name = file_path[-1]\n","        file_path = \"/\".join(file_path[:-1]) + \"/\"\n","\n","\n","        log_string = \"Compiling [{}/{}]: {}... \".format(file_index+1, len(cython_file_list), file_name)\n","        print(log_string)\n","\n","        try:\n","            run_compile_subprocess(file_path, [file_name])\n","\n","            log_string += \"PASS\\n\"\n","            print(log_string)\n","            log_file.write(log_string)\n","            log_file.flush()\n","\n","        except Exception as exc:\n","            traceback.print_exc()\n","\n","            fail_count += 1\n","            log_string += \"FAIL: {}\\n\".format(str(exc))\n","            print(log_string)\n","            log_file.write(log_string)\n","            log_file.flush()\n","\n","\n","    log_string = \"run_compile_all_cython: Compilation finished. \"\n","\n","    if fail_count != 0:\n","        log_string += \"FAILS {}/{}.\".format(fail_count, len(cython_file_list))\n","    else:\n","        log_string += \"SUCCESS.\"\n","\n","    log_string += \"\\nCompilation log can be found here: '{}'\".format(log_file_path)\n","\n","    print(log_string)\n","    log_file.write(log_string)\n","    log_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile ./Recommenders/Similarity/Cython/Compute_Similarity_Cython.pyx\n","\n","\"\"\"\n","Created on 23/10/17\n","@author: Maurizio Ferrari Dacrema\n","\"\"\"\n","\n","#cython: boundscheck=False\n","#cython: wraparound=True\n","#cython: initializedcheck=False\n","#cython: language_level=3\n","#cython: nonecheck=False\n","#cython: cdivision=True\n","#cython: unpack_method_calls=True\n","#cython: overflowcheck=False\n","\n","\"\"\"\n","Determine the operative system. The interface of numpy returns a different type for argsort under windows and linux\n","http://docs.cython.org/en/latest/src/userguide/language_basics.html#conditional-compilation\n","\"\"\"\n","IF UNAME_SYSNAME == \"linux\":\n","    DEF LONG_t = \"long\"\n","ELIF  UNAME_SYSNAME == \"Windows\":\n","    DEF LONG_t = \"long long\"\n","ELSE:\n","    DEF LONG_t = \"long long\"\n","\n","\n","\n","import time, sys\n","import cython\n","import numpy as np\n","cimport numpy as np\n","\n","from cpython.array cimport array, clone\n","\n","from libc.math cimport sqrt\n","\n","\n","\n","\n","import scipy.sparse as sps\n","from Recommenders.Recommender_utils import check_matrix\n","from Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n","\n","@cython.boundscheck(False)\n","@cython.wraparound(False)\n","@cython.initializedcheck(False)\n","@cython.nonecheck(False)\n","@cython.cdivision(True)\n","@cython.overflowcheck(False)\n","cdef class Compute_Similarity_Cython:\n","\n","    cdef int TopK \n","    cdef long n_columns, n_rows , end_col\n","\n","    cdef double[:] this_item_weights\n","    cdef int[:] this_item_weights_mask, this_item_weights_id\n","    cdef int this_item_weights_counter\n","\n","    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n","    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n","    cdef double[:] user_to_item_data, item_to_user_data\n","    cdef double[:] sum_of_squared, sum_of_squared_to_1_minus_alpha, sum_of_squared_to_alpha\n","    cdef int shrink, normalize, adjusted_cosine, pearson_correlation, tanimoto_coefficient, asymmetric_cosine, dice_coefficient, tversky_coefficient\n","    cdef float asymmetric_alpha, tversky_alpha, tversky_beta\n","\n","    cdef int use_row_weights\n","    cdef double[:] row_weights\n","\n","    cdef double[:,:] W_dense\n","\n","    def __init__(self, dataMatrix, topK = 100, shrink=0, normalize = True,\n","                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n","                 similarity = \"cosine\", row_weights = None):\n","        \"\"\"\n","        Computes the cosine similarity on the columns of dataMatrix\n","        If it is computed on URM=|users|x|items|, pass the URM as is.\n","        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n","        :param dataMatrix:\n","        :param topK:\n","        :param shrink:\n","        :param normalize:           If True divide the dot product by the product of the norms\n","        :param row_weights:         Multiply the values in each row by a specified value. Array\n","        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n","        :param similarity:  \"cosine\"        computes Cosine similarity\n","                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n","                            \"asymmetric\"    computes Asymmetric Cosine\n","                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n","                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n","                            \"dice\"          computes Dice similarity for binary interactions\n","                            \"tversky\"       computes Tversky similarity for binary interactions\n","                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n","        \"\"\"\n","        \"\"\"\n","        Asymmetric Cosine as described in: \n","        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n","        \n","        \"\"\"\n","\n","        super(Compute_Similarity_Cython, self).__init__()\n","\n","        self.n_columns = dataMatrix.shape[1]\n","        self.n_rows = dataMatrix.shape[0]\n","        self.shrink = shrink\n","        self.normalize = normalize\n","        self.asymmetric_alpha = asymmetric_alpha\n","        self.tversky_alpha = tversky_alpha\n","        self.tversky_beta = tversky_beta\n","\n","        self.adjusted_cosine = False\n","        self.asymmetric_cosine = False\n","        self.pearson_correlation = False\n","        self.tanimoto_coefficient = False\n","        self.dice_coefficient = False\n","        self.tversky_coefficient = False\n","\n","        if similarity == \"adjusted\":\n","            self.adjusted_cosine = True\n","        elif similarity == \"asymmetric\":\n","            self.asymmetric_cosine = True\n","        elif similarity == \"pearson\":\n","            self.pearson_correlation = True\n","        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n","            self.tanimoto_coefficient = True\n","            # Tanimoto has a specific kind of normalization\n","            self.normalize = False\n","\n","        elif similarity == \"dice\":\n","            self.dice_coefficient = True\n","            self.normalize = False\n","\n","        elif similarity == \"tversky\":\n","            self.tversky_coefficient = True\n","            self.normalize = False\n","\n","        elif similarity == \"cosine\":\n","            pass\n","        else:\n","            raise ValueError(\"Cosine_Similarity: value for parameter 'mode' not recognized.\"\n","                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n","                             \"dice, tversky.\"\n","                             \" Passed value was '{}'\".format(similarity))\n","\n","\n","        self.TopK = min(topK, self.n_columns)\n","        self.this_item_weights = np.zeros(self.n_columns, dtype=np.float64)\n","        self.this_item_weights_id = np.zeros(self.n_columns, dtype=np.int32)\n","        self.this_item_weights_mask = np.zeros(self.n_columns, dtype=np.int32)\n","        self.this_item_weights_counter = 0\n","\n","        # Copy data to avoid altering the original object\n","        dataMatrix = dataMatrix.copy()\n","\n","\n","\n","\n","\n","        if self.adjusted_cosine:\n","            dataMatrix = self.applyAdjustedCosine(dataMatrix)\n","        elif self.pearson_correlation:\n","            dataMatrix = self.applyPearsonCorrelation(dataMatrix)\n","        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n","            dataMatrix = self.useOnlyBooleanInteractions(dataMatrix)\n","\n","\n","\n","        # Compute sum of squared values to be used in normalization\n","        self.sum_of_squared = np.array(dataMatrix.power(2).sum(axis=0), dtype=np.float64).ravel()\n","\n","        # Tanimoto does not require the square root to be applied\n","        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n","            self.sum_of_squared = np.sqrt(self.sum_of_squared)\n","\n","        if self.asymmetric_cosine:\n","            # The power of 1-alpha may be negative so add small value to ensure values are non-zeros\n","            sum_of_squared_np = np.array(self.sum_of_squared) + 1e-6\n","            self.sum_of_squared_to_alpha = np.power(sum_of_squared_np, 2 * self.asymmetric_alpha)\n","            self.sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared_np, 2 * (1 - self.asymmetric_alpha))\n","\n","        # Apply weight after sum_of_squared has been computed but before the matrix is\n","        # split in its inner data structures\n","        self.use_row_weights = False\n","\n","        if row_weights is not None:\n","\n","            if dataMatrix.shape[0] != len(row_weights):\n","                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n","                                 \"Row_weights has {} rows, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n","\n","\n","            self.use_row_weights = True\n","            self.row_weights = np.array(row_weights, dtype=np.float64)\n","\n","\n","\n","\n","\n","        dataMatrix = check_matrix(dataMatrix, 'csr')\n","\n","        self.user_to_item_row_ptr = dataMatrix.indptr\n","        self.user_to_item_cols = dataMatrix.indices\n","        self.user_to_item_data = np.array(dataMatrix.data, dtype=np.float64)\n","\n","        dataMatrix = check_matrix(dataMatrix, 'csc')\n","        self.item_to_user_rows = dataMatrix.indices\n","        self.item_to_user_col_ptr = dataMatrix.indptr\n","        self.item_to_user_data = np.array(dataMatrix.data, dtype=np.float64)\n","\n","\n","\n","\n","        if self.TopK == 0:\n","            self.W_dense = np.zeros((self.n_columns,self.n_columns))\n","\n","\n","\n","\n","\n","    cdef useOnlyBooleanInteractions(self, dataMatrix):\n","        \"\"\"\n","        Set to 1 all data points\n","        :return:\n","        \"\"\"\n","\n","        cdef long index\n","\n","        for index in range(len(dataMatrix.data)):\n","            dataMatrix.data[index] = 1\n","\n","        return dataMatrix\n","\n","\n","\n","    cdef applyPearsonCorrelation(self, dataMatrix):\n","        \"\"\"\n","        Remove from every data point the average for the corresponding column\n","        :return:\n","        \"\"\"\n","\n","        cdef double[:] sumPerCol\n","        cdef int[:] interactionsPerCol\n","        cdef long colIndex, innerIndex, start_pos, end_pos\n","        cdef double colAverage\n","\n","\n","        dataMatrix = check_matrix(dataMatrix, 'csc')\n","\n","\n","        sumPerCol = np.array(dataMatrix.sum(axis=0), dtype=np.float64).ravel()\n","        interactionsPerCol = np.diff(dataMatrix.indptr)\n","\n","\n","        #Remove for every row the corresponding average\n","        for colIndex in range(self.n_columns):\n","\n","            if interactionsPerCol[colIndex]>0:\n","\n","                colAverage = sumPerCol[colIndex] / interactionsPerCol[colIndex]\n","\n","                start_pos = dataMatrix.indptr[colIndex]\n","                end_pos = dataMatrix.indptr[colIndex+1]\n","\n","                innerIndex = start_pos\n","\n","                while innerIndex < end_pos:\n","\n","                    dataMatrix.data[innerIndex] -= colAverage\n","                    innerIndex+=1\n","\n","\n","        return dataMatrix\n","\n","\n","\n","    cdef applyAdjustedCosine(self, dataMatrix):\n","        \"\"\"\n","        Remove from every data point the average for the corresponding row\n","        :return:\n","        \"\"\"\n","\n","        cdef double[:] sumPerRow\n","        cdef int[:] interactionsPerRow\n","        cdef long rowIndex, innerIndex, start_pos, end_pos\n","        cdef double rowAverage\n","\n","        dataMatrix = check_matrix(dataMatrix, 'csr')\n","\n","        sumPerRow = np.array(dataMatrix.sum(axis=1), dtype=np.float64).ravel()\n","        interactionsPerRow = np.diff(dataMatrix.indptr)\n","\n","\n","        #Remove for every row the corresponding average\n","        for rowIndex in range(self.n_rows):\n","\n","            if interactionsPerRow[rowIndex]>0:\n","\n","                rowAverage = sumPerRow[rowIndex] / interactionsPerRow[rowIndex]\n","\n","                start_pos = dataMatrix.indptr[rowIndex]\n","                end_pos = dataMatrix.indptr[rowIndex+1]\n","\n","                innerIndex = start_pos\n","\n","                while innerIndex < end_pos:\n","\n","                    dataMatrix.data[innerIndex] -= rowAverage\n","                    innerIndex+=1\n","\n","\n","        return dataMatrix\n","\n","\n","\n","\n","\n","    cdef int[:] getUsersThatRatedItem(self, long item_id):\n","        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n","\n","    cdef int[:] getItemsRatedByUser(self, long user_id):\n","        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n","\n","\n","\n","\n","    cdef computeItemSimilarities(self, long item_id_input):\n","        \"\"\"\n","        For every item the cosine similarity against other items depends on whether they have users in common. The more\n","        common users the higher the similarity.\n","        \n","        The basic implementation is:\n","        - Select the first item\n","        - Loop through all other items\n","        -- Given the two items, get the users they have in common\n","        -- Update the similarity for all common users\n","        \n","        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n","        \n","        A better way is to use the data structure in a different way skipping the search part, getting directly the\n","        information we need.\n","        \n","        The implementation here used is:\n","        - Select the first item\n","        - Initialize a zero valued array for the similarities\n","        - Get the users who rated the first item\n","        - Loop through the users\n","        -- Given a user, get the items he rated (second item)\n","        -- Update the similarity of the items he rated\n","        \n","        \n","        \"\"\"\n","\n","        # Create template used to initialize an array with zeros\n","        # Much faster than np.zeros(self.n_columns)\n","        #cdef array[double] template_zero = array('d')\n","        #cdef array[double] result = clone(template_zero, self.n_columns, zero=True)\n","\n","\n","        cdef long user_index, user_id, item_index, item_id, item_id_second\n","\n","        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n","        cdef int[:] items_rated_by_user\n","\n","        cdef double rating_item_input, rating_item_second, row_weight\n","\n","        # Clean previous item\n","        for item_index in range(self.this_item_weights_counter):\n","            item_id = self.this_item_weights_id[item_index]\n","            self.this_item_weights_mask[item_id] = False\n","            self.this_item_weights[item_id] = 0.0\n","\n","        self.this_item_weights_counter = 0\n","\n","\n","\n","        # Get users that rated the items\n","        for user_index in range(len(users_that_rated_item)):\n","\n","            user_id = users_that_rated_item[user_index]\n","            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n","\n","            if self.use_row_weights:\n","                row_weight = self.row_weights[user_id]\n","            else:\n","                row_weight = 1.0\n","\n","            # Get all items rated by that user\n","            items_rated_by_user = self.getItemsRatedByUser(user_id)\n","\n","            for item_index in range(len(items_rated_by_user)):\n","\n","                item_id_second = items_rated_by_user[item_index]\n","                if item_id_second>=self.end_col:\n","                    continue\n","                # Do not compute the similarity on the diagonal\n","                if item_id_second != item_id_input:\n","                    # Increment similairty\n","                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n","\n","                    self.this_item_weights[item_id_second] += rating_item_input*rating_item_second*row_weight\n","\n","\n","                    # Update global data structure\n","                    if not self.this_item_weights_mask[item_id_second]:\n","\n","                        self.this_item_weights_mask[item_id_second] = True\n","                        self.this_item_weights_id[self.this_item_weights_counter] = item_id_second\n","                        self.this_item_weights_counter += 1\n","\n","\n","\n","\n","    def compute_similarity(self, start_col=None, end_col=None):\n","        \"\"\"\n","        Compute the similarity for the given dataset\n","        :param self:\n","        :param start_col: column to begin with\n","        :param end_col: column to stop before, end_col is excluded\n","        :return:\n","        \"\"\"\n","\n","        cdef int print_block_size = 500\n","\n","        cdef int item_index, inner_item_index, item_id, local_topK\n","        cdef long long topK_item_index\n","\n","        cdef long long[:] top_k_idx\n","\n","        # Declare numpy data type to use vector indexing and simplify the topK selection code\n","        cdef np.ndarray[LONG_t, ndim=1] top_k_partition, top_k_partition_sorting\n","        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np = np.zeros(self.n_columns, dtype=np.float64)\n","        #cdef double[:] this_item_weights\n","\n","        cdef long processed_items = 0\n","\n","        # Data structure to incrementally build sparse matrix\n","        # Preinitialize max possible length\n","        cdef unsigned long long max_cells = <long long> self.n_columns*self.TopK\n","        cdef double[:] values = np.zeros((max_cells))\n","        cdef int[:] rows = np.zeros((max_cells,), dtype=np.int32)\n","        cdef int[:] cols = np.zeros((max_cells,), dtype=np.int32)\n","        cdef long sparse_data_pointer = 0\n","\n","        cdef int start_col_local = 0, end_col_local = self.n_columns\n","\n","        cdef array[double] template_zero = array('d')\n","        if end_col is None:\n","            self.end_col=self.n_columns\n","            start_col=0\n","        else:\n","            self.end_col=end_col\n","            start_col=end_col\n","        \n","        end_col=None\n","\n","        if start_col is not None and start_col>0 and start_col<self.n_columns:\n","            start_col_local = start_col\n","\n","        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n","            end_col_local = end_col\n","\n","\n","        \n","\n","\n","\n","\n","        start_time = time.time()\n","        last_print_time = start_time\n","\n","        item_index = start_col_local\n","\n","        # Compute all similarities for each item\n","        while item_index < end_col_local:\n","\n","            processed_items += 1\n","\n","            # Computed similarities go in self.this_item_weights\n","            self.computeItemSimilarities(item_index)\n","\n","\n","            # Apply normalization and shrinkage, ensure denominator != 0\n","            if self.normalize:\n","                for inner_item_index in range(self.n_columns):\n","\n","                    if self.asymmetric_cosine:\n","                        self.this_item_weights[inner_item_index] /= self.sum_of_squared_to_alpha[item_index] * self.sum_of_squared_to_1_minus_alpha[inner_item_index]\\\n","                                                             + self.shrink + 1e-6\n","\n","                    else:\n","                        self.this_item_weights[inner_item_index] /= self.sum_of_squared[item_index] * self.sum_of_squared[inner_item_index]\\\n","                                                             + self.shrink + 1e-6\n","\n","            # Apply the specific denominator for Tanimoto\n","            elif self.tanimoto_coefficient:\n","                for inner_item_index in range(self.n_columns):\n","                    self.this_item_weights[inner_item_index] /= self.sum_of_squared[item_index] + self.sum_of_squared[inner_item_index] -\\\n","                                                         self.this_item_weights[inner_item_index] + self.shrink + 1e-6\n","\n","            elif self.dice_coefficient:\n","                for inner_item_index in range(self.n_columns):\n","                    self.this_item_weights[inner_item_index] /= self.sum_of_squared[item_index] + self.sum_of_squared[inner_item_index] +\\\n","                                                         self.shrink + 1e-6\n","\n","            elif self.tversky_coefficient:\n","                for inner_item_index in range(self.n_columns):\n","                    self.this_item_weights[inner_item_index] /= self.this_item_weights[inner_item_index] + \\\n","                                                              (self.sum_of_squared[item_index]-self.this_item_weights[inner_item_index])*self.tversky_alpha + \\\n","                                                              (self.sum_of_squared[inner_item_index]-self.this_item_weights[inner_item_index])*self.tversky_beta +\\\n","                                                              self.shrink + 1e-6\n","\n","            elif self.shrink != 0:\n","                for inner_item_index in range(self.n_columns):\n","                    self.this_item_weights[inner_item_index] /= self.shrink\n","\n","\n","            if self.TopK == 0:\n","\n","                for inner_item_index in range(self.n_columns):\n","                    self.W_dense[inner_item_index,item_index] = self.this_item_weights[inner_item_index]\n","\n","            else:\n","\n","                # Sort indices and select TopK\n","                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n","                #top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n","\n","                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n","                # because we avoid sorting elements we already know we don't care about\n","                # - Partition the data to extract the set of TopK items, this set is unsorted\n","                # - Sort only the TopK items, discarding the rest\n","                # - Get the original item index\n","                #\n","\n","\n","\n","                #this_item_weights_np = clone(template_zero, self.this_item_weights_counter, zero=False)\n","                for inner_item_index in range(self.n_columns):\n","                    this_item_weights_np[inner_item_index] = 0.0\n","\n","\n","                # Add weights in the same ordering as the self.this_item_weights_id data structure\n","                for inner_item_index in range(self.this_item_weights_counter):\n","                    item_id = self.this_item_weights_id[inner_item_index]\n","                    this_item_weights_np[inner_item_index] = - self.this_item_weights[item_id]\n","\n","                #this_item_weights_np[start_col:]=0.0\n","\n","                local_topK = min([self.TopK, self.this_item_weights_counter])\n","                \n","                # Get the unordered set of topK items\n","                top_k_partition = np.argpartition(this_item_weights_np, local_topK-1)[0:local_topK]\n","                # Sort only the elements in the partition\n","                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n","                # Get original index\n","                top_k_idx = top_k_partition[top_k_partition_sorting]\n","\n","\n","\n","                # Incrementally build sparse matrix, do not add zeros\n","                for inner_item_index in range(len(top_k_idx)):\n","\n","                    topK_item_index = top_k_idx[inner_item_index]\n","\n","                    item_id = self.this_item_weights_id[topK_item_index]\n","\n","                    if self.this_item_weights[item_id] != 0.0:\n","\n","                        values[sparse_data_pointer] = self.this_item_weights[item_id]\n","                        rows[sparse_data_pointer] = item_id\n","                        cols[sparse_data_pointer] = item_index\n","\n","                        sparse_data_pointer += 1\n","\n","\n","            item_index += 1\n","\n","\n","            if processed_items % print_block_size==0 or processed_items==end_col_local:\n","\n","                current_time = time.time()\n","\n","                # Set block size to the number of items necessary in order to print every 300 seconds\n","                if current_time - start_time != 0:\n","                    items_per_sec = processed_items/(current_time - start_time)\n","                else:\n","                    items_per_sec = 1\n","\n","                print_block_size = int(items_per_sec*60)\n","\n","                if current_time - last_print_time > 60  or processed_items==end_col_local:\n","                    new_time_value, new_time_unit = seconds_to_biggest_unit(time.time() - start_time)\n","\n","                    print(\"Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}\".format(\n","                        processed_items, processed_items*1.0/(end_col_local-start_col_local)*100, items_per_sec, new_time_value, new_time_unit))\n","\n","                    last_print_time = current_time\n","\n","                    sys.stdout.flush()\n","                    sys.stderr.flush()\n","\n","        # End while on columns\n","\n","\n","        if self.TopK == 0:\n","\n","            return np.array(self.W_dense)\n","\n","        else:\n","\n","            values = np.array(values[0:sparse_data_pointer])\n","            rows = np.array(rows[0:sparse_data_pointer])\n","            cols = np.array(cols[0:sparse_data_pointer])\n","\n","            W_sparse = sps.csr_matrix((values, (rows, cols)),\n","                                    shape=(self.n_columns, self.n_columns),\n","                                    dtype=np.float32)\n","\n","            return W_sparse"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile ./Recommenders/KNN/UserKNNCFRecommender.py \n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on 23/10/17\n","\n","@author: Maurizio Ferrari Dacrema\n","\"\"\"\n","\n","from Recommenders.Recommender_utils import check_matrix\n","from Recommenders.BaseSimilarityMatrixRecommender import BaseUserSimilarityMatrixRecommender\n","\n","from Recommenders.IR_feature_weighting import okapi_BM_25, TF_IDF\n","import numpy as np\n","\n","from Recommenders.Similarity.Compute_Similarity import Compute_Similarity\n","\n","\n","class UserKNNCFRecommender(BaseUserSimilarityMatrixRecommender):\n","    \"\"\" UserKNN recommender\"\"\"\n","\n","    RECOMMENDER_NAME = \"UserKNNCFRecommender\"\n","\n","    FEATURE_WEIGHTING_VALUES = [\"BM25\", \"TF-IDF\", \"none\"]\n","\n","\n","    def __init__(self, URM_train, verbose = True):\n","        super(UserKNNCFRecommender, self).__init__(URM_train, verbose = verbose)\n","\n","\n","    def fit(self, feature_weighting = 'none',start_user=None, **similarity_args):\n","\n","        self.topK = similarity_args[\"topK\"]\n","        self.shrink = similarity_args[\"shrink\"]\n","        if feature_weighting not in self.FEATURE_WEIGHTING_VALUES:\n","            raise ValueError(\"Value for 'feature_weighting' not recognized. Acceptable values are {}, provided was '{}'\".format(self.FEATURE_WEIGHTING_VALUES, feature_weighting))\n","\n","        similarity = Compute_Similarity(self.URM_train.T, **similarity_args)\n","\n","        self.W_sparse = similarity.compute_similarity(end_col=start_user)\n","        self.W_sparse = check_matrix(self.W_sparse, format='csr')\n","        \n","    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, items_to_compute = None,\n","                  remove_top_pop_flag = False, remove_custom_items_flag = False, return_scores = False):\n","        if np.isscalar(user_id_array):\n","            user_id_array = np.atleast_1d(user_id_array)\n","            single_user = True\n","        else:\n","            single_user = False\n","        user_id_array=user_id_array.copy()-self.offset\n","        #print(user_id_array)\n","        return super(UserKNNCFRecommender, self).recommend(user_id_array,cutoff,remove_seen_flag,items_to_compute,remove_top_pop_flag,remove_custom_items_flag ,return_scores)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile ./Recommenders/NonPersonalizedRecommender.py \n","   \n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","@author: Massimo Quadrana\n","\"\"\"\n","\n","import numpy as np\n","from Recommenders.BaseRecommender import BaseRecommender\n","from Recommenders.Recommender_utils import check_matrix\n","from Recommenders.DataIO import DataIO\n","\n","\n","class TopPop(BaseRecommender):\n","    \"\"\"Top Popular recommender\"\"\"\n","\n","    RECOMMENDER_NAME = \"TopPopRecommender\"\n","\n","    def __init__(self, URM_train):\n","        super(TopPop, self).__init__(URM_train)\n","\n","\n","    def fit(self,offset):\n","\n","        # Use np.ediff1d and NOT a sum done over the rows as there might be values other than 0/1\n","        self.item_pop = np.ediff1d(self.URM_train.tocsc().indptr)\n","        self.n_items = self.URM_train.shape[1]\n","        self.offset=offset\n","\n","\n","    def _compute_item_score(self, user_id_array, items_to_compute = None):\n","\n","        # Create a single (n_items, ) array with the item score, then copy it for every user\n","\n","        if items_to_compute is not None:\n","            item_pop_to_copy = - np.ones(self.n_items, dtype=np.float32)*np.inf\n","            item_pop_to_copy[items_to_compute] = self.item_pop[items_to_compute].copy()\n","        else:\n","            item_pop_to_copy = self.item_pop.copy()\n","\n","        item_scores = np.array(item_pop_to_copy, dtype=np.float32).reshape((1, -1))\n","        item_scores = np.repeat(item_scores, len(user_id_array), axis = 0)\n","\n","        return item_scores\n","\n","\n","    def save_model(self, folder_path, file_name = None):\n","\n","        if file_name is None:\n","            file_name = self.RECOMMENDER_NAME\n","\n","        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n","\n","        data_dict_to_save = {\"item_pop\": self.item_pop}\n","\n","        dataIO = DataIO(folder_path=folder_path)\n","        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n","\n","        self._print(\"Saving complete\")\n","    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, items_to_compute = None,\n","                  remove_top_pop_flag = False, remove_custom_items_flag = False, return_scores = False):\n","        if np.isscalar(user_id_array):\n","            user_id_array = np.atleast_1d(user_id_array)\n","            single_user = True\n","        else:\n","            single_user = False\n","        user_id_array=user_id_array.copy()-self.offset\n","        #print(user_id_array)\n","        return super(TopPop, self).recommend(user_id_array,cutoff,remove_seen_flag,items_to_compute,remove_top_pop_flag,remove_custom_items_flag ,return_scores)\n","\n","\n","\n","class GlobalEffects(BaseRecommender):\n","    \"\"\"GlobalEffects\"\"\"\n","\n","    RECOMMENDER_NAME = \"GlobalEffectsRecommender\"\n","\n","    def __init__(self, URM_train):\n","        super(GlobalEffects, self).__init__(URM_train)\n","\n","\n","    def fit(self, lambda_user=10, lambda_item=25):\n","\n","        self.lambda_user = lambda_user\n","        self.lambda_item = lambda_item\n","        self.n_items = self.URM_train.shape[1]\n","\n","        # convert to csc matrix for faster column-wise sum\n","        self.URM_train = check_matrix(self.URM_train, 'csc', dtype=np.float32)\n","\n","        # 1) global average\n","        self.mu = self.URM_train.data.sum(dtype=np.float32) / self.URM_train.nnz\n","\n","        # 2) item average bias\n","        # compute the number of non-zero elements for each column\n","        # it is equivalent to:\n","        # col_nnz = X.indptr[1:] - X.indptr[:-1]\n","        # and it is **much faster** than\n","        # col_nnz = (X != 0).sum(axis=0)\n","        col_nnz = np.ediff1d(self.URM_train.indptr)\n","\n","        URM_train_unbiased = self.URM_train.copy()\n","        URM_train_unbiased.data -= self.mu\n","        self.item_bias = URM_train_unbiased.sum(axis=0) / (col_nnz + self.lambda_item)\n","        self.item_bias = np.asarray(self.item_bias).ravel()  # converts 2-d matrix to 1-d array without anycopy\n","        self.item_bias[col_nnz==0] = -np.inf\n","\n","        # 3) user average bias\n","        # NOTE: the user bias is *useless* for the sake of ranking items. We just show it here for educational purposes.\n","\n","        # first subtract the item biases from each column\n","        # then repeat each element of the item bias vector a number of times equal to col_nnz\n","        # and subtract it from the data vector\n","        URM_train_unbiased.data -= np.repeat(self.item_bias, col_nnz)\n","\n","        # now convert the csc matrix to csr for efficient row-wise computation\n","        URM_train_unbiased_csr = URM_train_unbiased.tocsr()\n","        row_nnz = np.ediff1d(URM_train_unbiased_csr.indptr)\n","        # finally, let's compute the bias\n","        self.user_bias = URM_train_unbiased_csr.sum(axis=1).ravel() / (row_nnz + self.lambda_user)\n","        self.user_bias = np.asarray(self.user_bias).ravel()\n","        self.user_bias[row_nnz==0] = -np.inf\n","\n","        self.URM_train = check_matrix(self.URM_train, 'csr', dtype=np.float32)\n","\n","\n","\n","\n","    def _compute_item_score(self, user_id_array, items_to_compute=None):\n","\n","        # Create a single (n_items, ) array with the item score, then copy it for every user\n","        # 4) Compute the item ranking by using the item bias only\n","        # the global average and user bias won't change the ranking, so there is no need to use them\n","\n","        if items_to_compute is not None:\n","            item_bias_to_copy = - np.ones(self.n_items, dtype=np.float32)*np.inf\n","            item_bias_to_copy[items_to_compute] = self.item_bias[items_to_compute].copy()\n","        else:\n","            item_bias_to_copy = self.item_bias.copy()\n","\n","        item_scores = np.array(item_bias_to_copy, dtype=np.float).reshape((1, -1))\n","        item_scores = np.repeat(item_scores, len(user_id_array), axis = 0)\n","\n","        return item_scores\n","\n","\n","    def save_model(self, folder_path, file_name = None):\n","\n","        if file_name is None:\n","            file_name = self.RECOMMENDER_NAME\n","\n","        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n","\n","        data_dict_to_save = {\"item_bias\": self.item_bias}\n","\n","        dataIO = DataIO(folder_path=folder_path)\n","        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n","\n","        self._print(\"Saving complete\")\n","\n","\n","\n","class Random(BaseRecommender):\n","    \"\"\"Random recommender\"\"\"\n","\n","    RECOMMENDER_NAME = \"RandomRecommender\"\n","\n","    def __init__(self, URM_train):\n","        super(Random, self).__init__(URM_train)\n","\n","\n","    def fit(self, random_seed=42):\n","        np.random.seed(random_seed)\n","        self.n_items = self.URM_train.shape[1]\n","\n","\n","    def _compute_item_score(self, user_id_array, items_to_compute = None):\n","\n","        # Create a random block (len(user_id_array), n_items) array with the item score\n","\n","        if items_to_compute is not None:\n","            item_scores = - np.ones((len(user_id_array), self.n_items), dtype=np.float32)*np.inf\n","            item_scores[:, items_to_compute] = np.random.rand(len(user_id_array), len(items_to_compute))\n","\n","        else:\n","            item_scores = np.random.rand(len(user_id_array), self.n_items)\n","\n","        return item_scores\n","\n","\n","\n","    def save_model(self, folder_path, file_name = None):\n","\n","        if file_name is None:\n","            file_name = self.RECOMMENDER_NAME\n","\n","        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n","\n","        data_dict_to_save = {}\n","\n","        dataIO = DataIO(folder_path=folder_path)\n","        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n","\n","        self._print(\"Saving complete\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["!python ./run_compile_all_cython.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd ../"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%capture\n","import numpy as np \n","import pandas as pd \n","import os\n","import scipy.sparse as sps\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from Data_manager.split_functions.split_train_validation_random_holdout import *\n","from Evaluation.Evaluator import *\n","from Evaluation.metrics import *\n","from Evaluation.Evaluator import _create_empty_metrics_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from Recommenders.MatrixFactorization.IALSRecommender import IALSRecommender\n","from Recommenders.NonPersonalizedRecommender import TopPop, Random, GlobalEffects\n","from Recommenders.KNN.UserKNNCFRecommender import UserKNNCFRecommender\n","from Recommenders.KNN.ItemKNNCFRecommender import ItemKNNCFRecommender\n","from Recommenders.KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\n","from Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender\n","from Recommenders.GraphBased.P3alphaRecommender import P3alphaRecommender"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import os, multiprocessing\n","from functools import partial\n","import traceback, os\n","import scipy.sparse as sps"]},{"cell_type":"markdown","metadata":{},"source":["# HITRATE"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","class HITRATEEvaluator(EvaluatorHoldout):\n","    def __init__(self, URM_test_list, cutoff_list, min_ratings_per_user=1, exclude_seen=True,\n","                 diversity_object = None,\n","                 ignore_items = None,\n","                 ignore_users = None,\n","                 verbose=True):\n","\n","\n","        super(HITRATEEvaluator, self).__init__(URM_test_list, cutoff_list,\n","                                               diversity_object = diversity_object,\n","                                               min_ratings_per_user =min_ratings_per_user, exclude_seen=exclude_seen,\n","                                               ignore_items = ignore_items, ignore_users = ignore_users,\n","                                               verbose = verbose)\n","        \n","    def _run_evaluation_on_selected_users(self, recommender_objects, users_to_evaluate, block_size = None,items_to_compute=None,name=\"_\"):\n","\n","        if block_size is None:\n","            # Reduce block size if estimated memory requirement exceeds 4 GB\n","            block_size = min([4000, int(4*1e9*8/64/self.n_items), len(users_to_evaluate)])\n","\n","\n","        results_dict = _create_empty_metrics_dict(self.cutoff_list,\n","                                                  self.n_items, self.n_users,\n","                                                  recommender_objects[0].get_URM_train(),\n","                                                  self.URM_test,\n","                                                  self.ignore_items_ID,\n","                                                  self.ignore_users_ID,\n","                                                  self.diversity_object)\n","\n","\n","        \n","\n","        # Start from -block_size to ensure it to be 0 at the first block\n","        user_batch_start = 0\n","        user_batch_end = 0\n","\n","        while user_batch_start < len(users_to_evaluate):\n","\n","            user_batch_end = user_batch_start + block_size\n","            user_batch_end = min(user_batch_end, len(users_to_evaluate))\n","\n","            test_user_batch_array = np.array(users_to_evaluate[user_batch_start:user_batch_end])\n","            user_batch_start = user_batch_end\n","            recommended_items_batch_list = None\n","            # Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\n","            for recommender_object in recommender_objects:\n","                recommended_items_batch_list_single, score = recommender_object.recommend(test_user_batch_array,\n","                                                                          remove_seen_flag=self.exclude_seen,\n","                                                                          cutoff = self.max_cutoff,\n","                                                                          items_to_compute=items_to_compute,\n","                                                                          remove_top_pop_flag=False,\n","                                                                          remove_custom_items_flag=self.ignore_items_flag,\n","                                                                          return_scores = True\n","                                                                         )\n","                \n","                if recommended_items_batch_list is None:\n","                    recommended_items_batch_list = recommended_items_batch_list_single\n","                    score_sum=score\n","                else:\n","                    recommended_items_batch_list=np.hstack((recommended_items_batch_list,recommended_items_batch_list_single))\n","                    score_sum+=score\n","                \n","            recommended_items_batch_list\n","\n","            results_dict = self._compute_metrics_on_recommendation_list(test_user_batch_array = test_user_batch_array,\n","                                                         recommended_items_batch_list = recommended_items_batch_list,\n","                                                         score_batch=score_sum,\n","                                                         results_dict = results_dict,name=name)\n","\n","\n","        return results_dict\n","    \n","    def _compute_metrics_on_recommendation_list(self, test_user_batch_array, recommended_items_batch_list,score_batch,results_dict,name=\"_\"):\n","\n","        assert len(recommended_items_batch_list) == len(test_user_batch_array), \"{}: recommended_items_batch_list contained recommendations for {} users, expected was {}\".format(\n","            self.EVALUATOR_NAME, len(recommended_items_batch_list), len(test_user_batch_array))\n","\n","        temp=pd.DataFrame()\n","        col_session=None\n","        col_items=None\n","        col_score=None\n","        col_max_score=None\n","        length=0\n","        # Compute recommendation quality for each user in batch\n","        for batch_user_index,recommended_items in enumerate(recommended_items_batch_list):\n","            score=score_batch[batch_user_index]\n","            recommended_items=np.unique(recommended_items)\n","            scores_top=score[recommended_items]\n","            \n","            self.len_unique+=len(recommended_items)\n","            test_user = test_user_batch_array[batch_user_index]\n","            session_id= np.repeat(test_user,len(recommended_items))\n","            maximum_score= np.repeat(np.max(score),len(recommended_items))\n","            if col_session is None:\n","                col_session=session_id\n","                col_items=recommended_items\n","                col_score=scores_top/np.max(score)\n","                col_max_score=maximum_score\n","            else:\n","                col_session=np.append(col_session,session_id)\n","                col_items=np.append(col_items,recommended_items)\n","                col_score=np.append(col_score,scores_top/np.max(score))\n","                col_max_score=np.append(col_max_score,maximum_score)\n","            #print(recommended_items[-4:])\n","            #print(scores_top[-4:])\n","            #print(session_id[-4:])\n","            relevant_items = self.get_user_relevant_items(test_user)\n","\n","            # Being the URM CSR, the indices are the non-zero column indexes\n","            \n","            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n","\n","            self._n_users_evaluated += 1\n","\n","            cutoff = self.max_cutoff\n","\n","            results_current_cutoff = results_dict[cutoff]\n","            results_current_cutoff[EvaluatorMetrics.HIT_RATE.value].add_recommendations(is_relevant)\n","        if time.time() - self._start_time_print > 60 or self._n_users_evaluated==len(self.users_to_evaluate):\n","\n","            elapsed_time = time.time()-self._start_time\n","            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n","\n","            self._print(\"Processed {} ({:4.1f}%) in {:.2f} {}. Users per second: {:.0f}\".format(\n","                          self._n_users_evaluated,\n","                          100.0* float(self._n_users_evaluated)/len(self.users_to_evaluate),\n","                          new_time_value, new_time_unit,\n","                          float(self._n_users_evaluated)/elapsed_time if elapsed_time>0.0 else np.nan))\n","\n","            sys.stdout.flush()\n","            sys.stderr.flush()\n","\n","            self._start_time_print = time.time()\n","        temp['Session_Id']=col_session\n","        temp['Item_ID']=col_items\n","        temp[f'Score_{name}']=col_score\n","        temp[f'Max_Score_{name}']=col_max_score\n","        self.dataset_for_ranker=pd.concat([self.dataset_for_ranker, temp], ignore_index=True)\n","        #print(temp.tail(4))\n","        #print(self.dataset_for_ranker.tail(4))\n","\n","        return results_dict\n","    def evaluateRecommender(self, recommender_objects,block_size=None,items_to_compute=None,name=\"_\"):\n","        \"\"\"\n","        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n","        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n","        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n","        :return results_df: dataframe with index the cutoff and columns the metric\n","        :return results_run_string: printable result string\n","        \"\"\"\n","        self.len_unique=0\n","        if self.ignore_items_flag:\n","            for recommender_object in recommender_objects:\n","                recommender_object.set_items_to_ignore(self.ignore_items_ID)\n","\n","        self._start_time = time.time()\n","        self._start_time_print = time.time()\n","        self._n_users_evaluated = 0\n","        self.dataset_for_ranker=pd.DataFrame(columns=['Session_Id', 'Item_ID', f'Score_{name}',f'Max_Score_{name}'])\n","        results_dict = self._run_evaluation_on_selected_users(recommender_objects, self.users_to_evaluate,block_size=block_size,items_to_compute=items_to_compute,name=name)\n","        self.dataset_for_ranker.to_csv(f\"../dataset/candidates/traditional_recs/train/{name}.csv\",index=False)\n","\n","        if self._n_users_evaluated > 0:\n","\n","            for cutoff in self.cutoff_list:\n","                results_current_cutoff = results_dict[cutoff]\n","\n","                for key in results_current_cutoff.keys():\n","                    if key!=\"HIT_RATE\":\n","                        continue\n","                    value = results_current_cutoff[key]\n","\n","                    \n","                    results_current_cutoff[key] = value.get_metric_value()*self._n_users_evaluated\n","                    \n","                    #results_current_cutoff[key] = value/self._n_users_evaluated\n","\n","\n","        else:\n","            self._print(\"WARNING: No users had a sufficient number of relevant items\")\n","        \n","        if self.ignore_items_flag:\n","            recommender_object.reset_items_to_ignore()\n","\n","        results_df = pd.DataFrame(columns=results_dict[self.cutoff_list[0]].keys(),\n","                                  index=self.cutoff_list)\n","        results_df.index.rename(\"cutoff\", inplace = True)\n","\n","        for cutoff in results_dict.keys():\n","            results_df.loc[cutoff] = results_dict[cutoff]\n","        print(\"average number of different items for each session\",self.len_unique/self._n_users_evaluated)\n","        #results_run_string = get_result_string_df(results_df)\n","\n","        return results_df.loc[self.cutoff_list[0]][\"HIT_RATE\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","from tqdm import tqdm\n","import numpy as np\n","\n","from joblib import Parallel, delayed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def get_ICM(files_directory=\"../dataset/processed_data/\"):\n","    df_icm = pd.read_csv(filepath_or_buffer=os.path.join(files_directory, 'simplified_features_and_categories_30.csv'), sep=',', header=0)\n","    \n","    item_id_list = df_icm['item_id'].values\n","    feat_id_list = df_icm['feature_idx'].values\n","    rating_id_list = np.ones_like(feat_id_list)\n","    ICM_matrix = sps.csr_matrix((rating_id_list, (item_id_list, feat_id_list)))\n","    return ICM_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["URM_train = sps.load_npz(\"../dataset/processed_data/URM_WW_train_full.npz\")\n","URM_train.data=np.ones_like(URM_train.data)\n","URM_valid = sps.load_npz(\"../dataset/processed_data/URM_WW_valid_bought.npz\").tocsr()\n","\n","temp= sps.load_npz(\"../dataset/processed_data/URM_WW_valid_seen.npz\")\n","URM_after_train = temp+URM_train\n","        \n","ICM_all = get_ICM()\n","if just_checking_integrity:\n","    URM_train = URM_train[boundary:boundary_after]\n","    URM_valid = URM_valid[boundary:boundary_after]\n","    URM_after_train = URM_after_train[boundary:boundary_after]\n","\n","\n","\n","\n","params_ICF=[\n","    {\"topK\":334,\"shrink\":396, \"similarity\":'cosine', \"feature_weighting\" : 'none' ,           \"power\":1.2561991065561426, \"weight\" :0.6393597471969044},\n","    {\"topK\":236,\"shrink\":687, \"similarity\":'tanimoto', \"feature_weighting\" : 'none' ,           \"power\":0.5509167891518838, \"weight\" :0.23362677165465845},\n","    {\"topK\":186,\"shrink\":699, \"similarity\":'dice', \"feature_weighting\" : 'none' ,           \"power\":0.5454807130915313, \"weight\" :0.37150060696200815},\n","    {\"topK\":236,\"shrink\":700, \"similarity\":'jaccard', \"feature_weighting\" : 'none' ,           \"power\":0.5224371929702275, \"weight\" :0.4532502310580744},\n","    {\"topK\":194,\"shrink\":45, \"similarity\":'adjusted', \"feature_weighting\" : 'none' ,           \"power\":1.3923780294419528, \"weight\" :0.747093242387402},\n","    {\"topK\":348,\"shrink\":661, \"similarity\":'asymmetric', \"feature_weighting\" : 'none' ,           \"power\":1.2462156230754484, \"weight\" :0.6410862143760282, \"asymmetric_alpha\" :0.6586444014201189},\n","    {\"topK\":587,\"shrink\":675, \"similarity\":'tversky', \"feature_weighting\" : 'none' ,           \"power\":1.894411025512657, \"weight\" :0.5871629850378687, \"tversky_alpha\" : 0.0022760544555811006,\"tversky_beta\" : 0.4049955813941767},\n","    {\"topK\":299,\"shrink\":674, \"similarity\":'pearson', \"feature_weighting\" : 'none' ,           \"power\":1.892818062582165, \"weight\" :0.5806120640896252}\n","]\n","params_graph=[\n","{\"topK\":760 , \"alpha\":0.35376041951192866  , \"implicit\":False , \"power\":1.6642347425902089, \"weight\":0.23322573709955957},\n","{\"topK\":1434 , \"alpha\":0.37391558455295765, \"beta\":0.13809420287767862  , \"implicit\":True , \"power\":1.6030310290761034, \"weight\":0.733513865097587}\n","]\n","params_ICBF=[\n","    {\"topK\":124,\"shrink\":9, \"similarity\":'tversky', \"feature_weighting\" : 'TF-IDF' ,           \"power\":1.861087725409631,  \"tversky_alpha\" : 0.7689675774356587,\"tversky_beta\" : 0.9052725399846582},\n","    {\"topK\":127,\"shrink\":79, \"similarity\":'asymmetric', \"feature_weighting\" : 'none' ,           \"power\":1.8699713052217346,  \"asymmetric_alpha\" : 0.7611612156193225},\n","    {\"topK\":218,\"shrink\":773, \"similarity\":'adjusted', \"feature_weighting\" : 'BM25' ,           \"power\":0.18601468383405495},\n","    {\"topK\":127,\"shrink\":19, \"similarity\":'cosine', \"feature_weighting\" : 'none' ,           \"power\":1.8849223054440538},\n","    {\"topK\":492,\"shrink\":348, \"similarity\":'pearson', \"feature_weighting\" : 'TF-IDF' ,           \"power\":1.467718817406988},\n","    {\"topK\":139,\"shrink\":3, \"similarity\":'dice', \"feature_weighting\" : 'TF-IDF' ,           \"power\":1.8969043493328803},\n","    {\"topK\":153,\"shrink\":5, \"similarity\":'jaccard', \"feature_weighting\" : 'BM25' ,           \"power\":1.8125421495255112},\n","    {\"topK\":152,\"shrink\":4, \"similarity\":'tanimoto', \"feature_weighting\" : 'BM25' ,           \"power\":1.8224861269207941}\n","]\n","params_UCF=[\n","    {\"topK\":1184,\"shrink\":386, \"similarity\":'cosine',           \"power\":0.1327615452747038,\"split\" :809438, \"keep_dup\" :False},\n","{\"topK\":1070,\"shrink\":15, \"similarity\":'tanimoto',           \"power\":0.845380696129735,\"split\" :813333, \"keep_dup\" :False},\n","{\"topK\":1751,\"shrink\":5, \"similarity\":'dice',           \"power\":0.6757609944671665,\"split\" :768495, \"keep_dup\" :False},\n","{\"topK\":1624,\"shrink\":16, \"similarity\":'jaccard',           \"power\":1.4426671932509196,\"split\" :765230, \"keep_dup\" :False},\n","{\"topK\":5123,\"shrink\":129, \"similarity\":'adjusted',           \"power\":0.12005038535002555,\"split\" :760859, \"keep_dup\" :True},\n","{\"topK\":4318,\"shrink\":3, \"similarity\":'pearson',           \"power\":0.16747332134568205,\"split\" :757664, \"keep_dup\" :True},\n","{\"topK\":5918,\"shrink\":1, \"similarity\":'asymmetric',           \"power\":1.443558847537245,\"split\" :776618, \"keep_dup\" :False,\"asymmetric_alpha\":0.33399818943172976},\n","{\"topK\":1031,\"shrink\":3, \"similarity\":'tversky',           \"power\":0.9137962256579533,\"split\" :775536, \"keep_dup\" :False,\"tversky_alpha\":0.5927402991066659,\"tversky_beta\":0.9129868678836787}\n","]\n","\n","params=params_UCF+params_ICBF+params_ICF+params_graph\n","\n","use_stacking=[False]*16+[True]*10\n","duplicates=[param[\"keep_dup\"] if \"keep_dup\" in param else False for param in params ]\n","transpose=[True]*8+[False]*18\n","get_input=[lambda urm_train,urm,icm: {\"URM_train\":urm}]*8+[lambda urm_train,urm,icm: {\"URM_train\":urm,\"ICM_train\":icm}]*8+[lambda urm_train,urm,icm: {\"URM_train\":urm_train}]*10\n","\n","rec_classes=([UserKNNCFRecommender]*8)+([ItemKNNCBFRecommender]*8)+([ItemKNNCFRecommender]*8)+[P3alphaRecommender,RP3betaRecommender]\n","\n","recs=[]\n","\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["candidates=np.unique(URM_valid.indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["recs_top=[]\n","if just_checking_integrity:\n","    splits=[0]\n","else:\n","    splits=[850000]\n","for split in splits:\n","    recTop=TopPop(URM_train[split:])\n","    recTop.fit(split)\n","    recs_top.append(np.array([recTop]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#ev = HITRATEEvaluator(URM_valid,cutoff_list=[200])\n","#result=ev.evaluateRecommender(recs_top[0],block_size=4000,items_to_compute=candidates,name= \"tentative\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if not just_checking_integrity:\n","    assert URM_valid[-81618,2638]==1,\"should be 1, something went wrong with data preparation\""]},{"cell_type":"code","execution_count":48,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity column 23692 (100.0%), 809.32 column/sec. Elapsed time 29.27 sec\n","-------------------14-------------------\n","ItemKNNCBFRecommender: URM Detected 18686 (78.9%) items with no interactions.\n","ItemKNNCBFRecommender: ICM Detected 1 ( 0.0%) items with no features.\n","Similarity column 23692 (100.0%), 881.44 column/sec. Elapsed time 26.88 sec\n","-------------------15-------------------\n","ItemKNNCBFRecommender: URM Detected 18686 (78.9%) items with no interactions.\n","ItemKNNCBFRecommender: ICM Detected 1 ( 0.0%) items with no features.\n","Similarity column 23692 (100.0%), 868.99 column/sec. Elapsed time 27.26 sec\n","-------------------16-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 861.87 column/sec. Elapsed time 27.49 sec\n","-------------------17-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 875.13 column/sec. Elapsed time 27.07 sec\n","-------------------18-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 845.66 column/sec. Elapsed time 28.02 sec\n","-------------------19-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 872.21 column/sec. Elapsed time 27.16 sec\n","-------------------20-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 883.98 column/sec. Elapsed time 26.80 sec\n","-------------------21-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 771.84 column/sec. Elapsed time 30.70 sec\n","-------------------22-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 722.93 column/sec. Elapsed time 32.77 sec\n","-------------------23-------------------\n","ItemKNNCFRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","ItemKNNCFRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","Similarity column 23692 (100.0%), 757.66 column/sec. Elapsed time 31.27 sec\n","-------------------24-------------------\n","P3alphaRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","P3alphaRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n","-------------------25-------------------\n","RP3betaRecommender: URM Detected 6619 (25.8%) users with no interactions.\n","RP3betaRecommender: URM Detected 1 ( 0.0%) items with no interactions.\n"]}],"source":["for i, rec in enumerate(rec_classes):\n","    print(f\"-------------------{i}-------------------\")\n","    params_rec=params[i]\n","    params_recommender={k:v for k,v in params_rec.items()  if k not in [\"weight\",\"power\",\"split\",\"keep_dup\"]}\n","    p=params_rec[\"power\"] \n","    \n","    split=\"split\" in params_rec\n","    X=URM_train.copy()\n","    X_post=URM_after_train.copy()\n","    if not duplicates[i]:\n","        X.data=np.ones_like(X.data)\n","        X_post.data=np.ones_like(X_post.data)\n","    if use_stacking[i]:\n","        w=params_rec[\"weight\"]\n","        URM_temp=sps.vstack([X*w,ICM_all.T*(1-w)]).tocsr()\n","        URM_temp_after=sps.vstack([X_post*w,ICM_all.T*(1-w)]).tocsr()\n","    else:\n","        URM_temp=X\n","        URM_temp_after=X_post\n","        \n","    if split:\n","        split_point=params_rec[\"split\"]\n","        if just_checking_integrity:\n","            temp=URM_train.sum(axis=1)\n","        else:\n","            URM_temp=URM_temp[split_point:,:]\n","            temp=URM_train[split_point:,:].sum(axis=1)\n","        start_valid_user=np.sum(temp!=0)\n","        params_recommender[\"start_user\"]=start_valid_user\n","        if not just_checking_integrity:\n","            URM_temp_after=URM_temp_after[split_point:,:]\n","    \n","        \n","    rec=rec(**get_input[i](URM_temp,URM_temp_after,ICM_all))\n","    rec.fit(**params_recommender)\n","    if split:\n","        if not just_checking_integrity:\n","            rec.offset=split_point\n","        else:\n","            rec.offset=0\n","    \n","    rec.W_sparse.data=np.power(rec.W_sparse.data,p)\n","    if transpose[i]:\n","        print(\"transposed\")\n","        rec.W_sparse=rec.W_sparse.T\n","    rec.W_sparse=rec.W_sparse.astype(\"float32\").tocsr()\n","    rec.URM_train=URM_temp_after.astype(\"float32\")\n","    recs.append(rec)\n","    #rec.save_model(\"./\",file_name =f\"{rec.RECOMMENDER_NAME}-{i}.data\")\n","recs=np.array(recs)"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[],"source":["UCF=recs[:8]\n","ICBF=recs[8:16]\n","ICF=recs[16:24]\n","Graph=recs[24:]"]},{"cell_type":"code","execution_count":50,"metadata":{"trusted":true},"outputs":[],"source":["if just_checking_integrity:\n","    rec_groups=[UCF,ICBF,ICF,Graph,*recs_top]\n","    names=[\"UCF_WW\",\"ICBF_WW\",\"ICF_WW\",\"Graph_WW\",\"TopPop15_WW\"]\n","else:\n","    rec_groups=[UCF,ICBF,ICF,Graph,*recs_top]\n","    names=[\"UCF_WW\",\"ICBF_WW\",\"ICF_WW\",\"Graph_WW\",\"TopPop15_WW\"]"]},{"cell_type":"code","execution_count":51,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["EvaluatorHoldout: Ignoring 18382 (73.5%) Users that have less than 1 test interactions\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\pietr\\.conda\\envs\\sub_env\\lib\\site-packages\\ipykernel_launcher.py:103: RuntimeWarning: invalid value encountered in true_divide\n"]},{"name":"stdout","output_type":"stream","text":["EvaluatorHoldout: Processed 6618 (100.0%) in 25.49 sec. Users per second: 260\n","average number of different items for each session 209.8120278029616\n","EvaluatorHoldout: Ignoring 18382 (73.5%) Users that have less than 1 test interactions\n","EvaluatorHoldout: Processed 6618 (100.0%) in 23.97 sec. Users per second: 276\n","average number of different items for each session 191.69537624660018\n","EvaluatorHoldout: Ignoring 18382 (73.5%) Users that have less than 1 test interactions\n","EvaluatorHoldout: Processed 6618 (100.0%) in 26.75 sec. Users per second: 247\n","average number of different items for each session 244.41387126019944\n","EvaluatorHoldout: Ignoring 18382 (73.5%) Users that have less than 1 test interactions\n","EvaluatorHoldout: Processed 6618 (100.0%) in 8.91 sec. Users per second: 743\n","average number of different items for each session 123.63145965548505\n","EvaluatorHoldout: Ignoring 18382 (73.5%) Users that have less than 1 test interactions\n","EvaluatorHoldout: Processed 6618 (100.0%) in 4.10 sec. Users per second: 1616\n","average number of different items for each session 100.0\n"]}],"source":["for i,rec_group in enumerate(rec_groups):\n","    name=names[i]\n","    ev = HITRATEEvaluator(URM_valid,cutoff_list=[100])\n","    result=ev.evaluateRecommender(rec_group,block_size=4000,items_to_compute=candidates,name= name)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 ('sub_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"61f38fddaed46afda7c40a5798d4631266e18af1d335aa6f772dc3ed9b8ab549"}}},"nbformat":4,"nbformat_minor":4}
